{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed for reproducibility\n",
    "my_seed = 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "my_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = my_path + 'data/c5_e4_1_train.svm'\n",
    "test = my_path + 'data/c5_e4_1_test.svm'\n",
    "test_csv = my_path + 'data/c5_e3_1_test.csv'\n",
    "db = my_path + 'data/c5_e6_train.db'\n",
    "pred = my_path + 'data/pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pd = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hces-bag models options:\n",
    "\n",
    "*    sgd     : Stochastic Gradient Descent\n",
    "*    svc     : Support Vector Machines\n",
    "*    gbc     : Gradient Boosting Classifiers\n",
    "*    dtree   : Decision Trees\n",
    "*    forest  : Random Forests\n",
    "*    extra   : Extra Trees\n",
    "*    kmp     : KMeans->LogisticRegression Pipelines\n",
    "*    kernp   : Nystroem Approx->Logistic Regression Pipelines\n",
    "*    dl      : Deep Learning\n",
    "*    sm      : KNN Hot Deck (Statistical Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/hillclimbing set size: 326899\n",
      "              Test set size: 980700\n",
      "\n",
      "Building SGDClassifier models\n",
      "Building RandomForestClassifier models\n",
      "Building GradientBoostingClassifier models\n",
      "Building MLPClassifier models\n",
      "built 78 models\n",
      "\n",
      "fitting ensemble:\n",
      "EnsembleSelectionClassifier(bag_fraction=0.25,\n",
      "              db_file='/media/mourao/BACKUP/renda_presumida/data/c5_e6_train.db',\n",
      "              epsilon=0.0001, max_models=25,\n",
      "              models=[SGDClassifier(alpha=0.000819600824639, average=False, class_weight=None,\n",
      "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
      "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
      "       shuffle...e=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)],\n",
      "              n_bags=20, n_best=5, n_folds=5, prune_fraction=0.75,\n",
      "              random_state=None, score_metric='f1', use_bootstrap=True,\n",
      "              use_epsilon=False, verbose=True)\n",
      "\n",
      "Train set accuracy from best model: 0.91653\n",
      "Train set accuracy from final ensemble: 0.90692\n",
      "\n",
      " Test set accuracy from best model: 0.85344\n",
      "\n",
      " Test set classification report for best model:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.83      0.84    463936\n",
      "        1.0       0.85      0.88      0.86    516764\n",
      "\n",
      "avg / total       0.85      0.85      0.85    980700\n",
      "\n",
      " Test set accuracy from final ensemble: 0.85399\n",
      "\n",
      " Test set classification report for final ensemble:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.86      0.83      0.84    463936\n",
      "        1.0       0.85      0.88      0.86    516764\n",
      "\n",
      "avg / total       0.85      0.85      0.85    980700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "\n",
      "fitting models:\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      ".................................................50\n",
      "..../usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "./usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      ".......................\n",
      "\n",
      "scoring models:\n",
      ".................................................50\n",
      "............................\n",
      "19 models left after pruning\n",
      "leaving 4 candidates per bag\n",
      "\n",
      "Best model CV score: 0.81740\n",
      "Best model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=20, max_features='sqrt', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=1e-05,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Ensemble scores for each bag (size/score):\n",
      "Bag 01): 04/0.815 \n",
      "         05/0.816 06/0.817 07/0.817 08/0.817 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 02): 04/0.815 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.818 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 03): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.818 23/0.818 24/0.817 25/0.818 \n",
      "Bag 04): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.818 09/0.817 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 05): 04/0.817 \n",
      "         05/0.818 06/0.818 07/0.818 08/0.818 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 06): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 07): 04/0.815 \n",
      "         05/0.816 06/0.817 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 08): 04/0.814 \n",
      "         05/0.816 06/0.816 07/0.817 08/0.817 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 09): 04/0.815 \n",
      "         05/0.816 06/0.817 07/0.817 08/0.817 09/0.817 10/0.818 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.817 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 10): 04/0.814 \n",
      "         05/0.815 06/0.816 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 11): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 12): 04/0.814 \n",
      "         05/0.816 06/0.816 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.818 14/0.817 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 13): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 14): 04/0.815 \n",
      "         05/0.816 06/0.817 07/0.817 08/0.818 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.817 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 15): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.817 24/0.818 25/0.818 \n",
      "Bag 16): 04/0.807 \n",
      "         05/0.809 06/0.809 07/0.810 08/0.809 09/0.810 10/0.809 11/0.810 12/0.810 \n",
      "         13/0.810 14/0.810 15/0.810 16/0.810 17/0.810 18/0.810 19/0.810 20/0.810 \n",
      "         21/0.810 22/0.810 23/0.810 24/0.810 25/0.810 \n",
      "Bag 17): 04/0.812 \n",
      "         05/0.814 06/0.815 07/0.815 08/0.816 09/0.816 10/0.816 11/0.816 12/0.816 \n",
      "         13/0.816 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 18): 04/0.812 \n",
      "         05/0.814 06/0.815 07/0.816 08/0.817 09/0.817 10/0.817 11/0.817 12/0.817 \n",
      "         13/0.817 14/0.817 15/0.817 16/0.817 17/0.817 18/0.817 19/0.817 20/0.817 \n",
      "         21/0.817 22/0.817 23/0.817 24/0.817 25/0.817 \n",
      "Bag 19): 04/0.817 \n",
      "         05/0.817 06/0.818 07/0.818 08/0.818 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "Bag 20): 04/0.816 \n",
      "         05/0.817 06/0.817 07/0.817 08/0.817 09/0.818 10/0.818 11/0.818 12/0.818 \n",
      "         13/0.818 14/0.818 15/0.818 16/0.818 17/0.818 18/0.818 19/0.818 20/0.818 \n",
      "         21/0.818 22/0.818 23/0.818 24/0.818 25/0.818 \n",
      "\n",
      "Final ensemble (429 components) CV score: 0.81818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$db\" \"$train\"\n",
    "rm $1\n",
    "python pyensemble/ensemble_train.py -M sgd forest gbc dl -F 5 -S f1  $1 $2 -U -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.83      0.90   2556691\n",
      "          1       0.37      0.88      0.52    295770\n",
      "\n",
      "avg / total       0.92      0.83      0.86   2852461\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[2115323  441368]\n",
      " [  36230  259540]]\n"
     ]
    }
   ],
   "source": [
    "clas = !python pyensemble/ensemble_predict.py -s ens {db} {test}\n",
    "guess = [int(i) for i in clas if i == '0' or i == '1']\n",
    "fused = pd.concat([test_pd, pd.Series(guess, name='guess')], axis=1)  \n",
    "\n",
    "cm = confusion_matrix(fused['label'], fused['guess'])\n",
    "print(classification_report(fused['label'], fused['guess']))\n",
    "print('Confusion Matrix:\\n\\n ' + str(cm))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[2115323  441368]\n",
    " [  36230  259540]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = !python pyensemble/ensemble_predict.py -s ens -p {db} {test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.71835</td>\n",
       "      <td>0.28165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.56803</td>\n",
       "      <td>0.43197</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.87429</td>\n",
       "      <td>0.12571</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.71299</td>\n",
       "      <td>0.28701</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87452</td>\n",
       "      <td>0.12548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.90541</td>\n",
       "      <td>0.09459</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.92622</td>\n",
       "      <td>0.07378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.97406</td>\n",
       "      <td>0.02594</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.98822</td>\n",
       "      <td>0.01178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.87596</td>\n",
       "      <td>0.12404</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.97825</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.97376</td>\n",
       "      <td>0.02624</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.98308</td>\n",
       "      <td>0.01692</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.87382</td>\n",
       "      <td>0.12618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.98413</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.95188</td>\n",
       "      <td>0.04812</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.88930</td>\n",
       "      <td>0.11070</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.97662</td>\n",
       "      <td>0.02338</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.60028</td>\n",
       "      <td>0.39972</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.92589</td>\n",
       "      <td>0.07411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.89073</td>\n",
       "      <td>0.10927</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.98538</td>\n",
       "      <td>0.01462</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.83492</td>\n",
       "      <td>0.16508</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.84334</td>\n",
       "      <td>0.15666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.91305</td>\n",
       "      <td>0.08695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.85899</td>\n",
       "      <td>0.14101</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.94864</td>\n",
       "      <td>0.05136</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.78652</td>\n",
       "      <td>0.21348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.87204</td>\n",
       "      <td>0.12796</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.97817</td>\n",
       "      <td>0.02183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852431</th>\n",
       "      <td>0.00313</td>\n",
       "      <td>0.99687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852432</th>\n",
       "      <td>0.00326</td>\n",
       "      <td>0.99674</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852433</th>\n",
       "      <td>0.01198</td>\n",
       "      <td>0.98802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852434</th>\n",
       "      <td>0.00481</td>\n",
       "      <td>0.99519</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852435</th>\n",
       "      <td>0.00817</td>\n",
       "      <td>0.99183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852436</th>\n",
       "      <td>0.00306</td>\n",
       "      <td>0.99694</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852437</th>\n",
       "      <td>0.02284</td>\n",
       "      <td>0.97716</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852438</th>\n",
       "      <td>0.01120</td>\n",
       "      <td>0.98880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852439</th>\n",
       "      <td>0.00979</td>\n",
       "      <td>0.99021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852440</th>\n",
       "      <td>0.03727</td>\n",
       "      <td>0.96273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852441</th>\n",
       "      <td>0.00776</td>\n",
       "      <td>0.99224</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852442</th>\n",
       "      <td>0.00644</td>\n",
       "      <td>0.99356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852443</th>\n",
       "      <td>0.01056</td>\n",
       "      <td>0.98944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852444</th>\n",
       "      <td>0.00532</td>\n",
       "      <td>0.99468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852445</th>\n",
       "      <td>0.00497</td>\n",
       "      <td>0.99503</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852446</th>\n",
       "      <td>0.01423</td>\n",
       "      <td>0.98577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852447</th>\n",
       "      <td>0.02621</td>\n",
       "      <td>0.97379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852448</th>\n",
       "      <td>0.00783</td>\n",
       "      <td>0.99217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852449</th>\n",
       "      <td>0.01506</td>\n",
       "      <td>0.98494</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852450</th>\n",
       "      <td>0.01518</td>\n",
       "      <td>0.98482</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852451</th>\n",
       "      <td>0.00842</td>\n",
       "      <td>0.99158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852452</th>\n",
       "      <td>0.00577</td>\n",
       "      <td>0.99423</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852453</th>\n",
       "      <td>0.01731</td>\n",
       "      <td>0.98269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852454</th>\n",
       "      <td>0.01203</td>\n",
       "      <td>0.98797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852455</th>\n",
       "      <td>0.01393</td>\n",
       "      <td>0.98607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852456</th>\n",
       "      <td>0.00525</td>\n",
       "      <td>0.99475</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852457</th>\n",
       "      <td>0.00655</td>\n",
       "      <td>0.99345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852458</th>\n",
       "      <td>0.11328</td>\n",
       "      <td>0.88672</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852459</th>\n",
       "      <td>0.01471</td>\n",
       "      <td>0.98529</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852460</th>\n",
       "      <td>0.09972</td>\n",
       "      <td>0.90028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2852461 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              p0       p1  label\n",
       "0        0.71835  0.28165      0\n",
       "1        0.56803  0.43197      0\n",
       "2        0.87429  0.12571      0\n",
       "3        0.71299  0.28701      0\n",
       "4        0.87452  0.12548      0\n",
       "5        0.90541  0.09459      0\n",
       "6        0.92622  0.07378      0\n",
       "7        0.97406  0.02594      0\n",
       "8        0.98822  0.01178      0\n",
       "9        0.87596  0.12404      0\n",
       "10       0.97825  0.02175      0\n",
       "11       0.97376  0.02624      0\n",
       "12       0.98308  0.01692      0\n",
       "13       0.87382  0.12618      0\n",
       "14       0.98413  0.01587      0\n",
       "15       0.95188  0.04812      0\n",
       "16       0.88930  0.11070      0\n",
       "17       0.97662  0.02338      0\n",
       "18       0.60028  0.39972      0\n",
       "19       0.92589  0.07411      0\n",
       "20       0.89073  0.10927      0\n",
       "21       0.98538  0.01462      0\n",
       "22       0.83492  0.16508      0\n",
       "23       0.84334  0.15666      0\n",
       "24       0.91305  0.08695      0\n",
       "25       0.85899  0.14101      0\n",
       "26       0.94864  0.05136      0\n",
       "27       0.78652  0.21348      0\n",
       "28       0.87204  0.12796      0\n",
       "29       0.97817  0.02183      0\n",
       "...          ...      ...    ...\n",
       "2852431  0.00313  0.99687      1\n",
       "2852432  0.00326  0.99674      1\n",
       "2852433  0.01198  0.98802      1\n",
       "2852434  0.00481  0.99519      1\n",
       "2852435  0.00817  0.99183      1\n",
       "2852436  0.00306  0.99694      1\n",
       "2852437  0.02284  0.97716      1\n",
       "2852438  0.01120  0.98880      1\n",
       "2852439  0.00979  0.99021      1\n",
       "2852440  0.03727  0.96273      1\n",
       "2852441  0.00776  0.99224      1\n",
       "2852442  0.00644  0.99356      1\n",
       "2852443  0.01056  0.98944      1\n",
       "2852444  0.00532  0.99468      1\n",
       "2852445  0.00497  0.99503      1\n",
       "2852446  0.01423  0.98577      1\n",
       "2852447  0.02621  0.97379      1\n",
       "2852448  0.00783  0.99217      1\n",
       "2852449  0.01506  0.98494      1\n",
       "2852450  0.01518  0.98482      1\n",
       "2852451  0.00842  0.99158      1\n",
       "2852452  0.00577  0.99423      1\n",
       "2852453  0.01731  0.98269      1\n",
       "2852454  0.01203  0.98797      1\n",
       "2852455  0.01393  0.98607      1\n",
       "2852456  0.00525  0.99475      1\n",
       "2852457  0.00655  0.99345      1\n",
       "2852458  0.11328  0.88672      1\n",
       "2852459  0.01471  0.98529      1\n",
       "2852460  0.09972  0.90028      1\n",
       "\n",
       "[2852461 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "for i, l in enumerate(pred):\n",
    "    try:\n",
    "        neg, pos = l.split()\n",
    "        \n",
    "        neg = float(neg)\n",
    "        pos = float(pos)\n",
    "        \n",
    "        ls.append({'p0': neg, 'p1': pos}) \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "predictions = pd.DataFrame(ls)\n",
    "predictions = pd.concat([predictions, fused['label']], axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(predictions['label'], predictions['p1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285991291632691"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_auc = auc(fpr, tpr)\n",
    "my_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1d5ae7190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, label='AUC=%0.4f' % my_auc, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"center\",  frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
