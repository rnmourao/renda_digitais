{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "from pyensemble.statmatch.knn_hotdeck import KNNHotDeckClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def bot_message(text):\n",
    "    import telepot\n",
    "    \n",
    "    bot = telepot.Bot(\"522378190:AAFw_ZB90Xmtve0tMCcVuCmj0lvUQ5wcn9Y\")\n",
    "\n",
    "    message = bot.getUpdates()\n",
    "\n",
    "    bot.sendMessage(message[0]['message']['from']['id'], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed for reproducibility\n",
    "my_seed = 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "my_path = '../'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = my_path + 'data/c2_e6_2_train.svm'\n",
    "test = my_path + 'data/c2_e6_2_test.svm'\n",
    "test_csv = my_path + 'data/c2_e6_1_test.csv'\n",
    "db = my_path + 'data/c2_e7_4_train.db'\n",
    "pred = my_path + 'data/pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pd = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hces-bag models options:\n",
    "\n",
    "*    sgd     : Stochastic Gradient Descent\n",
    "*    svc     : Support Vector Machines\n",
    "*    gbc     : Gradient Boosting Classifiers\n",
    "*    dtree   : Decision Trees\n",
    "*    forest  : Random Forests\n",
    "*    extra   : Extra Trees\n",
    "*    kmp     : KMeans->LogisticRegression Pipelines\n",
    "*    kernp   : Nystroem Approx->Logistic Regression Pipelines\n",
    "*    dl      : Deep Learning\n",
    "*    sm      : KNN Hot Deck (Statistical Matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/hillclimbing set size: 7916\n",
      "              Test set size: 23748\n",
      "\n",
      "Building SGDClassifier models\n",
      "Building RandomForestClassifier models\n",
      "Building GradientBoostingClassifier models\n",
      "Building MLPClassifier models\n",
      "built 133 models\n",
      "\n",
      "fitting ensemble:\n",
      "EnsembleSelectionClassifier(bag_fraction=0.25,\n",
      "              db_file='/media/mourao/BACKUP/renda_presumida/data/c2_e7_4_train.db',\n",
      "              epsilon=0.0001, max_models=25,\n",
      "              models=[SGDClassifier(alpha=0.000450164151318, average=False, class_weight=None,\n",
      "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0,\n",
      "       learning_rate='optimal', loss='log', max_iter=None, n_iter=None,\n",
      "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
      "       shuffle...e=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
      "       verbose=False, warm_start=False)],\n",
      "              n_bags=20, n_best=5, n_folds=5, prune_fraction=0.75,\n",
      "              random_state=None, score_metric='f1', use_bootstrap=True,\n",
      "              use_epsilon=False, verbose=True)\n",
      "\n",
      "Train set accuracy from best model: 0.99747\n",
      "Train set accuracy from final ensemble: 0.99773\n",
      "\n",
      " Test set accuracy from best model: 0.97103\n",
      "\n",
      " Test set classification report for best model:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.95      0.97     11848\n",
      "        1.0       0.95      1.00      0.97     11900\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23748\n",
      "\n",
      " Test set accuracy from final ensemble: 0.97276\n",
      "\n",
      " Test set classification report for final ensemble:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.95      0.97     11848\n",
      "        1.0       0.95      1.00      0.97     11900\n",
      "\n",
      "avg / total       0.97      0.97      0.97     23748\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "\n",
      "fitting models:\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      ".................................................50\n",
      ".................................................100\n",
      "........./usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "./usr/local/lib/python2.7/dist-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      ".......................\n",
      "\n",
      "scoring models:\n",
      "...................../usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "............................50\n",
      ".................................................100\n",
      ".................................\n",
      "33 models left after pruning\n",
      "leaving 8 candidates per bag\n",
      "\n",
      "Best model CV score: 0.91224\n",
      "Best model: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=20, max_features=7, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=1e-05,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "\n",
      "Ensemble scores for each bag (size/score):\n",
      "Bag 01): 05/0.894 06/0.896 07/0.899 08/0.900 09/0.909 10/0.909 11/0.909 12/0.910 \n",
      "         13/0.910 14/0.911 15/0.911 16/0.911 17/0.912 18/0.912 19/0.912 20/0.912 \n",
      "         21/0.912 22/0.912 23/0.912 24/0.912 25/0.912 \n",
      "Bag 02): 05/0.912 06/0.913 07/0.914 08/0.915 09/0.915 10/0.915 11/0.915 12/0.915 \n",
      "         13/0.915 14/0.915 15/0.915 16/0.915 17/0.915 18/0.916 19/0.916 20/0.916 \n",
      "         21/0.916 22/0.916 23/0.916 24/0.916 25/0.916 \n",
      "Bag 03): 05/0.910 06/0.911 07/0.911 08/0.912 09/0.912 10/0.913 11/0.913 12/0.913 \n",
      "         13/0.913 14/0.913 15/0.913 16/0.913 17/0.913 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.914 22/0.914 23/0.914 24/0.914 25/0.914 \n",
      "Bag 04): 05/0.896 06/0.898 07/0.901 08/0.904 09/0.908 10/0.909 11/0.910 12/0.910 \n",
      "         13/0.910 14/0.911 15/0.911 16/0.912 17/0.912 18/0.913 19/0.913 20/0.912 \n",
      "         21/0.912 22/0.913 23/0.913 24/0.913 25/0.913 \n",
      "Bag 05): 05/0.908 06/0.910 07/0.910 08/0.911 09/0.911 10/0.912 11/0.912 12/0.912 \n",
      "         13/0.913 14/0.913 15/0.913 16/0.913 17/0.913 18/0.913 19/0.913 20/0.913 \n",
      "         21/0.913 22/0.913 23/0.913 24/0.914 25/0.914 \n",
      "Bag 06): 05/0.913 06/0.913 07/0.915 08/0.915 09/0.915 10/0.915 11/0.915 12/0.915 \n",
      "         13/0.914 14/0.914 15/0.914 16/0.915 17/0.915 18/0.915 19/0.915 20/0.915 \n",
      "         21/0.915 22/0.915 23/0.915 24/0.915 25/0.915 \n",
      "Bag 07): 05/0.897 06/0.899 07/0.911 08/0.913 09/0.913 10/0.913 11/0.914 12/0.914 \n",
      "         13/0.913 14/0.914 15/0.914 16/0.914 17/0.913 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.914 22/0.914 23/0.913 24/0.913 25/0.913 \n",
      "Bag 08): 05/0.907 06/0.910 07/0.912 08/0.911 09/0.912 10/0.912 11/0.912 12/0.912 \n",
      "         13/0.913 14/0.913 15/0.913 16/0.913 17/0.913 18/0.913 19/0.913 20/0.913 \n",
      "         21/0.913 22/0.913 23/0.913 24/0.913 25/0.913 \n",
      "Bag 09): 05/0.912 06/0.914 07/0.914 08/0.915 09/0.915 10/0.915 11/0.915 12/0.915 \n",
      "         13/0.915 14/0.915 15/0.915 16/0.915 17/0.915 18/0.915 19/0.915 20/0.915 \n",
      "         21/0.915 22/0.915 23/0.915 24/0.915 25/0.915 \n",
      "Bag 10): 05/0.896 06/0.897 07/0.902 08/0.908 09/0.912 10/0.912 11/0.913 12/0.914 \n",
      "         13/0.914 14/0.914 15/0.914 16/0.914 17/0.914 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.914 22/0.914 23/0.914 24/0.914 25/0.914 \n",
      "Bag 11): 05/0.902 06/0.906 07/0.907 08/0.909 09/0.911 10/0.911 11/0.911 12/0.911 \n",
      "         13/0.912 14/0.912 15/0.912 16/0.912 17/0.912 18/0.912 19/0.912 20/0.912 \n",
      "         21/0.912 22/0.912 23/0.912 24/0.912 25/0.912 \n",
      "Bag 12): 05/0.912 06/0.913 07/0.914 08/0.914 09/0.914 10/0.915 11/0.915 12/0.915 \n",
      "         13/0.915 14/0.915 15/0.915 16/0.915 17/0.915 18/0.915 19/0.915 20/0.915 \n",
      "         21/0.915 22/0.915 23/0.915 24/0.915 25/0.915 \n",
      "Bag 13): 05/0.902 06/0.908 07/0.910 08/0.911 09/0.911 10/0.911 11/0.912 12/0.913 \n",
      "         13/0.913 14/0.913 15/0.913 16/0.913 17/0.913 18/0.913 19/0.913 20/0.913 \n",
      "         21/0.913 22/0.913 23/0.913 24/0.913 25/0.913 \n",
      "Bag 14): 05/0.909 06/0.911 07/0.911 08/0.911 09/0.911 10/0.912 11/0.912 12/0.912 \n",
      "         13/0.912 14/0.912 15/0.912 16/0.912 17/0.912 18/0.912 19/0.912 20/0.913 \n",
      "         21/0.913 22/0.913 23/0.913 24/0.913 25/0.912 \n",
      "Bag 15): 05/0.912 06/0.913 07/0.914 08/0.914 09/0.914 10/0.914 11/0.914 12/0.914 \n",
      "         13/0.914 14/0.914 15/0.914 16/0.914 17/0.914 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.914 22/0.914 23/0.914 24/0.914 25/0.914 \n",
      "Bag 16): 05/0.896 06/0.897 07/0.900 08/0.904 09/0.906 10/0.907 11/0.909 12/0.909 \n",
      "         13/0.910 14/0.910 15/0.911 16/0.912 17/0.911 18/0.911 19/0.912 20/0.912 \n",
      "         21/0.912 22/0.912 23/0.912 24/0.912 25/0.912 \n",
      "Bag 17): 05/0.912 06/0.913 07/0.914 08/0.914 09/0.915 10/0.915 11/0.915 12/0.915 \n",
      "         13/0.915 14/0.915 15/0.915 16/0.915 17/0.915 18/0.915 19/0.915 20/0.915 \n",
      "         21/0.915 22/0.915 23/0.915 24/0.915 25/0.915 \n",
      "Bag 18): 05/0.906 06/0.911 07/0.912 08/0.913 09/0.913 10/0.914 11/0.914 12/0.914 \n",
      "         13/0.914 14/0.914 15/0.914 16/0.914 17/0.915 18/0.915 19/0.914 20/0.915 \n",
      "         21/0.915 22/0.914 23/0.914 24/0.914 25/0.914 \n",
      "Bag 19): 05/0.913 06/0.913 07/0.913 08/0.913 09/0.914 10/0.914 11/0.914 12/0.914 \n",
      "         13/0.914 14/0.914 15/0.914 16/0.915 17/0.914 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.914 22/0.914 23/0.914 24/0.914 25/0.914 \n",
      "Bag 20): 05/0.914 06/0.914 07/0.913 08/0.913 09/0.914 10/0.914 11/0.914 12/0.913 \n",
      "         13/0.913 14/0.914 15/0.914 16/0.914 17/0.914 18/0.914 19/0.914 20/0.914 \n",
      "         21/0.915 22/0.915 23/0.915 24/0.914 25/0.914 \n",
      "\n",
      "Final ensemble (399 components) CV score: 0.91407\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$db\" \"$train\"\n",
    "rm $1\n",
    "python pyensemble/ensemble_train.py -M sgd forest gbc dl -F 5 -S f1  $1 $2 -U -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.95      0.97     11897\n",
      "          1       0.39      0.72      0.51       580\n",
      "\n",
      "avg / total       0.96      0.94      0.94     12477\n",
      "\n",
      "Confusion Matrix:\n",
      "\n",
      " [[11258   639]\n",
      " [  165   415]]\n"
     ]
    }
   ],
   "source": [
    "clas = !python pyensemble/ensemble_predict.py -s ens {db} {test}\n",
    "guess = [int(i) for i in clas if i == '0' or i == '1']\n",
    "fused = pd.concat([test_pd, pd.Series(guess, name='guess')], axis=1)  \n",
    "\n",
    "cm = confusion_matrix(fused['label'], fused['guess'])\n",
    "print(classification_report(fused['label'], fused['guess']))\n",
    "print('Confusion Matrix:\\n\\n ' + str(cm))\n",
    "\n",
    "# bot_message('HCES-Bag: lr forest gbc dl: \\n' + str(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = !python pyensemble/ensemble_predict.py -s ens -p {db} {test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.97759</td>\n",
       "      <td>0.02241</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.86105</td>\n",
       "      <td>0.13895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98707</td>\n",
       "      <td>0.01293</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.97668</td>\n",
       "      <td>0.02332</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.82341</td>\n",
       "      <td>0.17659</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.33841</td>\n",
       "      <td>0.66159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.48634</td>\n",
       "      <td>0.51366</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.89959</td>\n",
       "      <td>0.10041</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.71052</td>\n",
       "      <td>0.28948</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.92055</td>\n",
       "      <td>0.07945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.69798</td>\n",
       "      <td>0.30202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.84967</td>\n",
       "      <td>0.15033</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.94467</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.87626</td>\n",
       "      <td>0.12374</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.95285</td>\n",
       "      <td>0.04715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.96075</td>\n",
       "      <td>0.03925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.77006</td>\n",
       "      <td>0.22994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.97435</td>\n",
       "      <td>0.02565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.69009</td>\n",
       "      <td>0.30991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.32137</td>\n",
       "      <td>0.67863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.90461</td>\n",
       "      <td>0.09539</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.80360</td>\n",
       "      <td>0.19640</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.98003</td>\n",
       "      <td>0.01997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.33265</td>\n",
       "      <td>0.66735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.92168</td>\n",
       "      <td>0.07832</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.88685</td>\n",
       "      <td>0.11315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.45999</td>\n",
       "      <td>0.54001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.36339</td>\n",
       "      <td>0.63661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.89217</td>\n",
       "      <td>0.10783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.83962</td>\n",
       "      <td>0.16038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>0.76384</td>\n",
       "      <td>0.23616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12448</th>\n",
       "      <td>0.94717</td>\n",
       "      <td>0.05283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12449</th>\n",
       "      <td>0.03919</td>\n",
       "      <td>0.96081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12450</th>\n",
       "      <td>0.50285</td>\n",
       "      <td>0.49715</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12451</th>\n",
       "      <td>0.84581</td>\n",
       "      <td>0.15419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12452</th>\n",
       "      <td>0.35177</td>\n",
       "      <td>0.64823</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12453</th>\n",
       "      <td>0.92892</td>\n",
       "      <td>0.07108</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12454</th>\n",
       "      <td>0.98026</td>\n",
       "      <td>0.01974</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12455</th>\n",
       "      <td>0.98425</td>\n",
       "      <td>0.01575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12456</th>\n",
       "      <td>0.76093</td>\n",
       "      <td>0.23907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12457</th>\n",
       "      <td>0.98131</td>\n",
       "      <td>0.01869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12458</th>\n",
       "      <td>0.94104</td>\n",
       "      <td>0.05896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12459</th>\n",
       "      <td>0.91714</td>\n",
       "      <td>0.08286</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12460</th>\n",
       "      <td>0.79402</td>\n",
       "      <td>0.20598</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12461</th>\n",
       "      <td>0.96446</td>\n",
       "      <td>0.03554</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>0.90903</td>\n",
       "      <td>0.09097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12463</th>\n",
       "      <td>0.90220</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12464</th>\n",
       "      <td>0.62732</td>\n",
       "      <td>0.37268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12465</th>\n",
       "      <td>0.90066</td>\n",
       "      <td>0.09934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12466</th>\n",
       "      <td>0.98099</td>\n",
       "      <td>0.01901</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12467</th>\n",
       "      <td>0.97198</td>\n",
       "      <td>0.02802</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12468</th>\n",
       "      <td>0.84094</td>\n",
       "      <td>0.15906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12469</th>\n",
       "      <td>0.96923</td>\n",
       "      <td>0.03077</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12470</th>\n",
       "      <td>0.50810</td>\n",
       "      <td>0.49190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12471</th>\n",
       "      <td>0.84314</td>\n",
       "      <td>0.15686</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12472</th>\n",
       "      <td>0.95339</td>\n",
       "      <td>0.04661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12473</th>\n",
       "      <td>0.66002</td>\n",
       "      <td>0.33998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>0.71884</td>\n",
       "      <td>0.28116</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12475</th>\n",
       "      <td>0.95889</td>\n",
       "      <td>0.04111</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12476</th>\n",
       "      <td>0.98570</td>\n",
       "      <td>0.01430</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12477 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            p0       p1  label\n",
       "0      0.97759  0.02241      0\n",
       "1      0.86105  0.13895      0\n",
       "2      0.98707  0.01293      0\n",
       "3      0.97668  0.02332      0\n",
       "4      0.82341  0.17659      0\n",
       "5      0.33841  0.66159      0\n",
       "6      0.48634  0.51366      0\n",
       "7      0.89959  0.10041      0\n",
       "8      0.71052  0.28948      0\n",
       "9      0.92055  0.07945      0\n",
       "10     0.69798  0.30202      0\n",
       "11     0.84967  0.15033      0\n",
       "12     0.94467  0.05533      0\n",
       "13     0.87626  0.12374      0\n",
       "14     0.95285  0.04715      0\n",
       "15     0.96075  0.03925      0\n",
       "16     0.77006  0.22994      0\n",
       "17     0.97435  0.02565      0\n",
       "18     0.69009  0.30991      0\n",
       "19     0.32137  0.67863      0\n",
       "20     0.90461  0.09539      0\n",
       "21     0.80360  0.19640      0\n",
       "22     0.98003  0.01997      0\n",
       "23     0.33265  0.66735      0\n",
       "24     0.92168  0.07832      0\n",
       "25     0.88685  0.11315      0\n",
       "26     0.45999  0.54001      0\n",
       "27     0.36339  0.63661      0\n",
       "28     0.89217  0.10783      0\n",
       "29     0.83962  0.16038      0\n",
       "...        ...      ...    ...\n",
       "12447  0.76384  0.23616      0\n",
       "12448  0.94717  0.05283      0\n",
       "12449  0.03919  0.96081      0\n",
       "12450  0.50285  0.49715      0\n",
       "12451  0.84581  0.15419      0\n",
       "12452  0.35177  0.64823      1\n",
       "12453  0.92892  0.07108      0\n",
       "12454  0.98026  0.01974      0\n",
       "12455  0.98425  0.01575      0\n",
       "12456  0.76093  0.23907      0\n",
       "12457  0.98131  0.01869      0\n",
       "12458  0.94104  0.05896      0\n",
       "12459  0.91714  0.08286      0\n",
       "12460  0.79402  0.20598      0\n",
       "12461  0.96446  0.03554      0\n",
       "12462  0.90903  0.09097      0\n",
       "12463  0.90220  0.09780      0\n",
       "12464  0.62732  0.37268      0\n",
       "12465  0.90066  0.09934      0\n",
       "12466  0.98099  0.01901      0\n",
       "12467  0.97198  0.02802      0\n",
       "12468  0.84094  0.15906      0\n",
       "12469  0.96923  0.03077      0\n",
       "12470  0.50810  0.49190      0\n",
       "12471  0.84314  0.15686      0\n",
       "12472  0.95339  0.04661      0\n",
       "12473  0.66002  0.33998      0\n",
       "12474  0.71884  0.28116      0\n",
       "12475  0.95889  0.04111      0\n",
       "12476  0.98570  0.01430      0\n",
       "\n",
       "[12477 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = []\n",
    "for i, l in enumerate(pred):\n",
    "    try:\n",
    "        neg, pos = l.split()\n",
    "        \n",
    "        neg = float(neg)\n",
    "        pos = float(pos)\n",
    "        \n",
    "        ls.append({'p0': neg, 'p1': pos}) \n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "predictions = pd.DataFrame(ls)\n",
    "predictions = pd.concat([predictions, fused['label']], axis=1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(predictions['label'], predictions['p1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9388406378890071"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_auc = auc(fpr, tpr)\n",
    "my_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d20177410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='blue',\n",
    "         lw=lw, label='AUC=%0.4f' % my_auc, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"center\",  frameon=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
