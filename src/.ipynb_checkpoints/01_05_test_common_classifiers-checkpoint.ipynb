{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_171\"; OpenJDK Runtime Environment (build 1.8.0_171-8u171-b11-0ubuntu0.16.04.1-b11); OpenJDK 64-Bit Server VM (build 25.171-b11, mixed mode)\n",
      "  Starting server from /usr/local/lib/python2.7/dist-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpoGIUIx\n",
      "  JVM stdout: /tmp/tmpoGIUIx/h2o_mourao_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpoGIUIx/h2o_mourao_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/Sao_Paulo</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.4</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 months and 24 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_mourao_jamnjl</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.12 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       America/Sao_Paulo\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.4\n",
       "H2O cluster version age:    2 months and 24 days\n",
       "H2O cluster name:           H2O_from_python_mourao_jamnjl\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.556 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             2.7.12 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "import h2o\n",
    "\n",
    "try:\n",
    "    h2o.cluster().shutdown()\n",
    "except AttributeError:\n",
    "    pass\n",
    "    \n",
    "h2o.init(nthreads = -1,\n",
    "         max_mem_size = \"4G\")\n",
    "#          max_mem_size = \"24G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seed for reproducibility\n",
    "my_seed = 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "my_path = '../'\n",
    "\n",
    "train = pd.read_csv(my_path + 'data/c2_e6_2_train.csv')\n",
    "test = pd.read_csv(my_path + 'data/c2_e6_1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove unused attributes\n",
    "\n",
    "train.drop('base_guess', axis=1, inplace=True)\n",
    "test.drop(['cd_pss', 'nm_mun_uor', 'sg_uf_uor', 'target', 'base_guess'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# transform data to H2O format.\n",
    "train_h2o = h2o.H2OFrame(train)\n",
    "test_h2o = h2o.H2OFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changing label to factor informs to H2O that the problem is solved by classification.\n",
    "train_h2o['label'] = train_h2o['label'].asfactor()\n",
    "test_h2o['label'] = test_h2o['label'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of features\n",
    "features = train.columns.tolist()\n",
    "features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "     alpha        model_ids                  f1\n",
      "0    [0.1]  lr_grid_model_1  0.8347987616099071\n",
      "1    [0.0]  lr_grid_model_0  0.8343077497201143\n",
      "2    [0.5]  lr_grid_model_2  0.8337945028592512\n",
      "3    [0.7]  lr_grid_model_3  0.8331144875883199\n",
      "4    [1.0]  lr_grid_model_4  0.8319325124833298\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.107862551509\n",
      "RMSE: 0.328424346706\n",
      "LogLoss: 0.375572969033\n",
      "Null degrees of freedom: 12476\n",
      "Residual degrees of freedom: 12427\n",
      "Null deviance: 17296.7947437\n",
      "Residual deviance: 9372.04786924\n",
      "AIC: 9472.04786924\n",
      "AUC: 0.897141556985\n",
      "Gini: 0.79428311397\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.844612541977: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11599.0</td>\n",
       "<td>298.0</td>\n",
       "<td>0.025</td>\n",
       "<td> (298.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>276.0</td>\n",
       "<td>304.0</td>\n",
       "<td>0.4759</td>\n",
       "<td> (276.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11875.0</td>\n",
       "<td>602.0</td>\n",
       "<td>0.046</td>\n",
       "<td> (574.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11599  298  0.025    (298.0/11897.0)\n",
       "1      276    304  0.4759   (276.0/580.0)\n",
       "Total  11875  602  0.046    (574.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.8446125</td>\n",
       "<td>0.5143824</td>\n",
       "<td>43.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.7574453</td>\n",
       "<td>0.5522247</td>\n",
       "<td>64.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9560257</td>\n",
       "<td>0.5692220</td>\n",
       "<td>14.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9874275</td>\n",
       "<td>0.9624910</td>\n",
       "<td>5.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9997644</td>\n",
       "<td>0.864</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0352667</td>\n",
       "<td>1.0</td>\n",
       "<td>383.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9997644</td>\n",
       "<td>0.9985711</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9079308</td>\n",
       "<td>0.4939335</td>\n",
       "<td>27.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4225921</td>\n",
       "<td>0.8206897</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4281826</td>\n",
       "<td>0.8217716</td>\n",
       "<td>165.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.844613     0.514382  43\n",
       "max f2                       0.757445     0.552225  64\n",
       "max f0point5                 0.956026     0.569222  14\n",
       "max accuracy                 0.987427     0.962491  5\n",
       "max precision                0.999764     0.864     0\n",
       "max recall                   0.0352667    1         383\n",
       "max specificity              0.999764     0.998571  0\n",
       "max absolute_mcc             0.907931     0.493933  27\n",
       "max min_per_class_accuracy   0.422592     0.82069   167\n",
       "max mean_per_class_accuracy  0.428183     0.821772  165"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9988865</td>\n",
       "<td>18.7585241</td>\n",
       "<td>18.7585241</td>\n",
       "<td>0.872</td>\n",
       "<td>0.872</td>\n",
       "<td>0.1879310</td>\n",
       "<td>0.1879310</td>\n",
       "<td>1775.8524138</td>\n",
       "<td>1775.8524138</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9668401</td>\n",
       "<td>11.7025655</td>\n",
       "<td>15.2305448</td>\n",
       "<td>0.544</td>\n",
       "<td>0.708</td>\n",
       "<td>0.1172414</td>\n",
       "<td>0.3051724</td>\n",
       "<td>1070.2565517</td>\n",
       "<td>1423.0544828</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.9213672</td>\n",
       "<td>9.4653103</td>\n",
       "<td>13.3088</td>\n",
       "<td>0.44</td>\n",
       "<td>0.6186667</td>\n",
       "<td>0.0948276</td>\n",
       "<td>0.4</td>\n",
       "<td>846.5310345</td>\n",
       "<td>1230.88</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.8767107</td>\n",
       "<td>7.0559586</td>\n",
       "<td>11.7455897</td>\n",
       "<td>0.328</td>\n",
       "<td>0.546</td>\n",
       "<td>0.0706897</td>\n",
       "<td>0.4706897</td>\n",
       "<td>605.5958621</td>\n",
       "<td>1074.5589655</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.8331605</td>\n",
       "<td>5.5515017</td>\n",
       "<td>10.5147132</td>\n",
       "<td>0.2580645</td>\n",
       "<td>0.4887821</td>\n",
       "<td>0.0551724</td>\n",
       "<td>0.5258621</td>\n",
       "<td>455.1501669</td>\n",
       "<td>951.4713196</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.6552013</td>\n",
       "<td>2.9303299</td>\n",
       "<td>6.7225216</td>\n",
       "<td>0.1362179</td>\n",
       "<td>0.3125</td>\n",
       "<td>0.1465517</td>\n",
       "<td>0.6724138</td>\n",
       "<td>193.0329907</td>\n",
       "<td>572.2521552</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.5323348</td>\n",
       "<td>1.4479277</td>\n",
       "<td>4.9643236</td>\n",
       "<td>0.0673077</td>\n",
       "<td>0.2307692</td>\n",
       "<td>0.0724138</td>\n",
       "<td>0.7448276</td>\n",
       "<td>44.7927719</td>\n",
       "<td>396.4323607</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.4359109</td>\n",
       "<td>1.3100298</td>\n",
       "<td>4.0507502</td>\n",
       "<td>0.0608974</td>\n",
       "<td>0.1883013</td>\n",
       "<td>0.0655172</td>\n",
       "<td>0.8103448</td>\n",
       "<td>31.0029841</td>\n",
       "<td>305.0750166</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.3039130</td>\n",
       "<td>0.7245444</td>\n",
       "<td>2.9426073</td>\n",
       "<td>0.0336808</td>\n",
       "<td>0.1367887</td>\n",
       "<td>0.0724138</td>\n",
       "<td>0.8827586</td>\n",
       "<td>-27.5455576</td>\n",
       "<td>194.2607350</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.2235344</td>\n",
       "<td>0.3447447</td>\n",
       "<td>2.2930116</td>\n",
       "<td>0.0160256</td>\n",
       "<td>0.1065919</td>\n",
       "<td>0.0344828</td>\n",
       "<td>0.9172414</td>\n",
       "<td>-65.5255305</td>\n",
       "<td>129.3011559</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.1708020</td>\n",
       "<td>0.1723723</td>\n",
       "<td>1.8688157</td>\n",
       "<td>0.0080128</td>\n",
       "<td>0.0868729</td>\n",
       "<td>0.0172414</td>\n",
       "<td>0.9344828</td>\n",
       "<td>-82.7627653</td>\n",
       "<td>86.8815736</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.1350074</td>\n",
       "<td>0.2760169</td>\n",
       "<td>1.6034911</td>\n",
       "<td>0.0128308</td>\n",
       "<td>0.0745391</td>\n",
       "<td>0.0275862</td>\n",
       "<td>0.9620690</td>\n",
       "<td>-72.3983077</td>\n",
       "<td>60.3491114</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.1093880</td>\n",
       "<td>0.2240841</td>\n",
       "<td>1.4063878</td>\n",
       "<td>0.0104167</td>\n",
       "<td>0.0653767</td>\n",
       "<td>0.0224138</td>\n",
       "<td>0.9844828</td>\n",
       "<td>-77.5915948</td>\n",
       "<td>40.6387838</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0846833</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2371433</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0575093</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9896552</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>23.7143331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0591662</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.1034778</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0512958</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>10.3477756</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0013851</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0068966</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.998886           18.7585    18.7585            0.872            0.872                       0.187931        0.187931                   1775.85   1775.85\n",
       "    2        0.0200369                   0.96684            11.7026    15.2305            0.544            0.708                       0.117241        0.305172                   1070.26   1423.05\n",
       "    3        0.0300553                   0.921367           9.46531    13.3088            0.44             0.618667                    0.0948276       0.4                        846.531   1230.88\n",
       "    4        0.0400737                   0.876711           7.05596    11.7456            0.328            0.546                       0.0706897       0.47069                    605.596   1074.56\n",
       "    5        0.050012                    0.833161           5.5515     10.5147            0.258065         0.488782                    0.0551724       0.525862                   455.15    951.471\n",
       "    6        0.100024                    0.655201           2.93033    6.72252            0.136218         0.3125                      0.146552        0.672414                   193.033   572.252\n",
       "    7        0.150036                    0.532335           1.44793    4.96432            0.0673077        0.230769                    0.0724138       0.744828                   44.7928   396.432\n",
       "    8        0.200048                    0.435911           1.31003    4.05075            0.0608974        0.188301                    0.0655172       0.810345                   31.003    305.075\n",
       "    9        0.299992                    0.303913           0.724544   2.94261            0.0336808        0.136789                    0.0724138       0.882759                   -27.5456  194.261\n",
       "    10       0.400016                    0.223534           0.344745   2.29301            0.0160256        0.106592                    0.0344828       0.917241                   -65.5255  129.301\n",
       "    11       0.50004                     0.170802           0.172372   1.86882            0.00801282       0.0868729                   0.0172414       0.934483                   -82.7628  86.8816\n",
       "    12       0.599984                    0.135007           0.276017   1.60349            0.0128308        0.0745391                   0.0275862       0.962069                   -72.3983  60.3491\n",
       "    13       0.700008                    0.109388           0.224084   1.40639            0.0104167        0.0653767                   0.0224138       0.984483                   -77.5916  40.6388\n",
       "    14       0.799952                    0.0846833          0.0517532  1.23714            0.00240577       0.0575093                   0.00517241      0.989655                   -94.8247  23.7143\n",
       "    15       0.899976                    0.0591662          0.0344745  1.10348            0.00160256       0.0512958                   0.00344828      0.993103                   -96.5526  10.3478\n",
       "    16       1                           0.00138513         0.0689489  1                  0.00320513       0.0464855                   0.00689655      1                          -93.1051  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression (LR)\n",
    "\n",
    "# lambda value | alpha value       | Result\n",
    "# -------------+-------------------+-------------------------------------\n",
    "# lambda = 0   | alpha = any value | No regularization. alpha is ignored.\n",
    "# lambda > 0   | alpha = 0         | Ridge Regression\n",
    "# lambda > 0   | alpha = 1         | LASSO\n",
    "# lambda > 0   | 0 < alpha < 1     | Elastic Net Penalty\n",
    "\n",
    "\n",
    "\n",
    "hyper_parameters = { 'alpha': [0, 0.1, 0.5, 0.7, 1.0] }\n",
    "\n",
    "lr_grid = H2OGridSearch(H2OGeneralizedLinearEstimator(family='binomial', \n",
    "                                                      seed=my_seed,\n",
    "                                                      fold_assignment='Modulo',\n",
    "                                                      lambda_search=True,\n",
    "                                                      nfolds=5), \n",
    "                         grid_id='lr_grid',\n",
    "                         hyper_params=hyper_parameters)\n",
    "\n",
    "lr_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "lr_gridperf1 = lr_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(lr_gridperf1)\n",
    "\n",
    "# Grab the top LR model, chosen by validation F1\n",
    "best_lr1 = lr_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_lr_perf1 = best_lr1.model_performance(test_h2o)\n",
    "print(best_lr_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkbiUKEREJC4lChERiUuJQkRE4vr/umktBjl9am8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f755b95e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_lr_perf1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.347159615932647,\n",
       " 2.138644424696164,\n",
       " 1.9486531483571814,\n",
       " 1.7755401733702547,\n",
       " 1.6178060779618149,\n",
       " 1.4740846448560774,\n",
       " 1.3431310277545856,\n",
       " 1.2238109690730978,\n",
       " 1.1150909755450116,\n",
       " 1.0160293665971023,\n",
       " 0.9257681179628905,\n",
       " 0.8435254298869167,\n",
       " 0.7685889555492659,\n",
       " 0.7003096310581943,\n",
       " 0.6380960535692057,\n",
       " 0.5814093588365343,\n",
       " 0.5297585538288361,\n",
       " 0.48269626398243803,\n",
       " 0.439814858256888,\n",
       " 0.4007429184298893,\n",
       " 0.36514202205034263,\n",
       " 0.33270381118497294,\n",
       " 0.3031473215694273,\n",
       " 0.2762165490302279,\n",
       " 0.2516782320990916,\n",
       " 0.2293198316136821,\n",
       " 0.20894768980506248,\n",
       " 0.19038535292674505,\n",
       " 0.17347204289675314,\n",
       " 0.15806126471478996,\n",
       " 0.14401953759262795,\n",
       " 0.1312252388073772,\n",
       " 0.11956755026364306,\n",
       " 0.1089454986401983,\n",
       " 0.09926708080737937,\n",
       " 0.09044846693999077,\n",
       " 0.0824132744234627,\n",
       " 0.07509190626418444,\n",
       " 0.06842094827364026,\n",
       " 0.062342619805045386,\n",
       " 0.05680427328502524,\n",
       " 0.051757938205521055,\n",
       " 0.04715990562619774,\n",
       " 0.0429703495885127,\n",
       " 0.03915298216231531,\n",
       " 0.035674739137155805,\n",
       " 0.03250549363591199,\n",
       " 0.029617795170192068,\n",
       " 0.026986631877336232,\n",
       " 0.0245892138796278,\n",
       " 0.02240477588927491,\n",
       " 0.020414397349421586,\n",
       " 0.018600838553335712,\n",
       " 0.016948391322316618,\n",
       " 0.015442742948965851,\n",
       " 0.014070852227362762,\n",
       " 0.012820836496377625,\n",
       " 0.01168186871774549,\n",
       " 0.010644083697440297,\n",
       " 0.009698492638083648,\n",
       " 0.008836905282282079,\n",
       " 0.008051858972535665,\n",
       " 0.007336554013268828,\n",
       " 0.006684794775120179,\n",
       " 0.0060909360313649245,\n",
       " 0.005549834061661605,\n",
       " 0.005056802099607208,\n",
       " 0.004607569738208696,\n",
       " 0.004198245941660559,\n",
       " 0.0038252853430541024,\n",
       " 0.0034854575361053606,\n",
       " 0.0031758190949212626,\n",
       " 0.002893688079452656,\n",
       " 0.002636620805812619,\n",
       " 0.002402390680255668,\n",
       " 0.0021889689134879198,\n",
       " 0.001994506948264781,\n",
       " 0.0018173204480724315,\n",
       " 0.0016558747082107124,\n",
       " 0.0015087713629152043,\n",
       " 0.0013747362733819407,\n",
       " 0.0012526084917866259,\n",
       " 0.0011413302057099682,\n",
       " 0.0010399375758725529,\n",
       " 0.000947552387820096,\n",
       " 0.0008633744452499716,\n",
       " 0.0007866746390936457,\n",
       " 0.000716788632322724,\n",
       " 0.0006531111057794253,\n",
       " 0.000595090515191616,\n",
       " 0.0005422243139601793,\n",
       " 0.0004940546003407872,\n",
       " 0.00045016415131804823]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_lr1.parms['lambda']['actual_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (RF)\n",
    "\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [50, 100],\n",
    "                 'mtries' : [1, -1],\n",
    "#                  'col_sample_rate_change_per_level' : [1, 2], # https://0xdata.atlassian.net/browse/PUBDEV-5334    \n",
    "                 'max_depth' : [2, 20],\n",
    "                 'sample_rate' : [0.6320000291, 1.],  # outro aumento absurdo\n",
    "                 'col_sample_rate_per_tree' : [0.5, 1.], # documentacao errada tb\n",
    "                 'min_split_improvement' : [1e-4, 1e-5] \n",
    "               }\n",
    "\n",
    "rf_grid = H2OGridSearch(H2ORandomForestEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                         grid_id='rf_grid',\n",
    "                         hyper_params=hyper_par)\n",
    "\n",
    "rf_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "rf_gridperf1 = rf_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# Grab the top RF model, chosen by validation F1\n",
    "best_rf = rf_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_rf_perf1 = best_rf.model_performance(test_h2o)\n",
    "print(best_rf_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (RF) - Refinement 1\n",
    "\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [75, 100],\n",
    "                 'mtries' : [-1],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1],  \n",
    "                 'col_sample_rate_per_tree' : [.5], \n",
    "                 'min_split_improvement' : [5e-4, 1e-5] \n",
    "               }\n",
    "\n",
    "rf_grid = H2OGridSearch(H2ORandomForestEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                         grid_id='rf_grid_1',\n",
    "                         hyper_params=hyper_par)\n",
    "\n",
    "rf_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "rf_gridperf1 = rf_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# Grab the top RF model, chosen by validation F1\n",
    "best_rf = rf_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_rf_perf1 = best_rf.model_performance(test_h2o)\n",
    "print(best_rf_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (RF) - Refinement 2\n",
    "\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [25, 100],\n",
    "                 'mtries' : [-1],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1],  \n",
    "                 'col_sample_rate_per_tree' : [.5], \n",
    "                 'min_split_improvement' : [7e-4, 1e-5] \n",
    "               }\n",
    "\n",
    "rf_grid = H2OGridSearch(H2ORandomForestEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                         grid_id='rf_grid_2',\n",
    "                         hyper_params=hyper_par)\n",
    "\n",
    "rf_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "rf_gridperf1 = rf_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# Grab the top RF model, chosen by validation F1\n",
    "best_rf = rf_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_rf_perf1 = best_rf.model_performance(test_h2o)\n",
    "print(best_rf_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest (RF) - Refinement 3\n",
    "\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [100, 150],\n",
    "                 'mtries' : [-1],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1],  \n",
    "                 'col_sample_rate_per_tree' : [.5], \n",
    "                 'min_split_improvement' : [8e-4, 1e-5] \n",
    "               }\n",
    "\n",
    "rf_grid = H2OGridSearch(H2ORandomForestEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                         grid_id='rf_grid_3',\n",
    "                         hyper_params=hyper_par)\n",
    "\n",
    "rf_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "rf_gridperf1 = rf_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# Grab the top RF model, chosen by validation F1\n",
    "best_rf = rf_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_rf_perf1 = best_rf.model_performance(test_h2o)\n",
    "print(best_rf_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    col_sample_rate_per_tree max_depth min_split_improvement mtries ntrees  \\\n",
      "0                        0.5        20                1.0E-5     -1    100   \n",
      "\n",
      "  sample_rate          model_ids                  f1  \n",
      "0         1.0  rf_grid_4_model_0  0.9995580529073805  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: drf\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0297861558489\n",
      "RMSE: 0.172586661851\n",
      "LogLoss: 0.119465735913\n",
      "Mean Per-Class Error: 0.140603759858\n",
      "AUC: 0.928242703898\n",
      "Gini: 0.856485407796\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.262218615992: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11609.0</td>\n",
       "<td>288.0</td>\n",
       "<td>0.0242</td>\n",
       "<td> (288.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>265.0</td>\n",
       "<td>315.0</td>\n",
       "<td>0.4569</td>\n",
       "<td> (265.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11874.0</td>\n",
       "<td>603.0</td>\n",
       "<td>0.0443</td>\n",
       "<td> (553.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11609  288  0.0242   (288.0/11897.0)\n",
       "1      265    315  0.4569   (265.0/580.0)\n",
       "Total  11874  603  0.0443   (553.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2622186</td>\n",
       "<td>0.5325444</td>\n",
       "<td>143.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1882609</td>\n",
       "<td>0.6052311</td>\n",
       "<td>178.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3946670</td>\n",
       "<td>0.5903548</td>\n",
       "<td>92.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4073942</td>\n",
       "<td>0.9631322</td>\n",
       "<td>88.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.97</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0001019</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.97</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2127157</td>\n",
       "<td>0.5132221</td>\n",
       "<td>165.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0949240</td>\n",
       "<td>0.8568966</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0982892</td>\n",
       "<td>0.8593962</td>\n",
       "<td>242.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.262219     0.532544  143\n",
       "max f2                       0.188261     0.605231  178\n",
       "max f0point5                 0.394667     0.590355  92\n",
       "max accuracy                 0.407394     0.963132  88\n",
       "max precision                0.97         1         0\n",
       "max recall                   0.000101935  1         399\n",
       "max specificity              0.97         1         0\n",
       "max absolute_mcc             0.212716     0.513222  165\n",
       "max min_per_class_accuracy   0.094924     0.856897  245\n",
       "max mean_per_class_accuracy  0.0982892    0.859396  242"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.6374441</td>\n",
       "<td>18.9306207</td>\n",
       "<td>18.9306207</td>\n",
       "<td>0.88</td>\n",
       "<td>0.88</td>\n",
       "<td>0.1896552</td>\n",
       "<td>0.1896552</td>\n",
       "<td>1793.0620690</td>\n",
       "<td>1793.0620690</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.4437804</td>\n",
       "<td>12.3909517</td>\n",
       "<td>15.6607862</td>\n",
       "<td>0.576</td>\n",
       "<td>0.728</td>\n",
       "<td>0.1241379</td>\n",
       "<td>0.3137931</td>\n",
       "<td>1139.0951724</td>\n",
       "<td>1466.0786207</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.3533212</td>\n",
       "<td>9.6374069</td>\n",
       "<td>13.6529931</td>\n",
       "<td>0.448</td>\n",
       "<td>0.6346667</td>\n",
       "<td>0.0965517</td>\n",
       "<td>0.4103448</td>\n",
       "<td>863.7406897</td>\n",
       "<td>1265.2993103</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.2970869</td>\n",
       "<td>7.7443448</td>\n",
       "<td>12.1758310</td>\n",
       "<td>0.36</td>\n",
       "<td>0.566</td>\n",
       "<td>0.0775862</td>\n",
       "<td>0.4879310</td>\n",
       "<td>674.4344828</td>\n",
       "<td>1117.5831034</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.2551347</td>\n",
       "<td>6.4189238</td>\n",
       "<td>11.0318302</td>\n",
       "<td>0.2983871</td>\n",
       "<td>0.5128205</td>\n",
       "<td>0.0637931</td>\n",
       "<td>0.5517241</td>\n",
       "<td>541.8923804</td>\n",
       "<td>1003.1830239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.1554884</td>\n",
       "<td>3.5853448</td>\n",
       "<td>7.3085875</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.3397436</td>\n",
       "<td>0.1793103</td>\n",
       "<td>0.7310345</td>\n",
       "<td>258.5344828</td>\n",
       "<td>630.8587533</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.1083511</td>\n",
       "<td>2.0684682</td>\n",
       "<td>5.5618811</td>\n",
       "<td>0.0961538</td>\n",
       "<td>0.2585470</td>\n",
       "<td>0.1034483</td>\n",
       "<td>0.8344828</td>\n",
       "<td>106.8468170</td>\n",
       "<td>456.1881079</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0833297</td>\n",
       "<td>0.8618617</td>\n",
       "<td>4.3868762</td>\n",
       "<td>0.0400641</td>\n",
       "<td>0.2039263</td>\n",
       "<td>0.0431034</td>\n",
       "<td>0.8775862</td>\n",
       "<td>-13.8138263</td>\n",
       "<td>338.6876243</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0554018</td>\n",
       "<td>0.5175317</td>\n",
       "<td>3.0977839</td>\n",
       "<td>0.0240577</td>\n",
       "<td>0.1440021</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.9293103</td>\n",
       "<td>-48.2468269</td>\n",
       "<td>209.7783909</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0394951</td>\n",
       "<td>0.3102702</td>\n",
       "<td>2.4007659</td>\n",
       "<td>0.0144231</td>\n",
       "<td>0.1116009</td>\n",
       "<td>0.0310345</td>\n",
       "<td>0.9603448</td>\n",
       "<td>-68.9729775</td>\n",
       "<td>140.0765861</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0284928</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.9343277</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0899183</td>\n",
       "<td>0.0068966</td>\n",
       "<td>0.9672414</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>93.4327727</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6009457</td>\n",
       "<td>0.02</td>\n",
       "<td>0.1708663</td>\n",
       "<td>1.6382224</td>\n",
       "<td>0.0079428</td>\n",
       "<td>0.0761536</td>\n",
       "<td>0.0172414</td>\n",
       "<td>0.9844828</td>\n",
       "<td>-82.9133686</td>\n",
       "<td>63.8222377</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0132973</td>\n",
       "<td>0.0348092</td>\n",
       "<td>1.4113139</td>\n",
       "<td>0.0016181</td>\n",
       "<td>0.0656057</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-96.5190827</td>\n",
       "<td>41.1313890</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0082253</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2414539</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0577096</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>24.1453935</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9000561</td>\n",
       "<td>0.0033012</td>\n",
       "<td>0.0172234</td>\n",
       "<td>1.1052951</td>\n",
       "<td>0.0008006</td>\n",
       "<td>0.0513802</td>\n",
       "<td>0.0017241</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-98.2776566</td>\n",
       "<td>10.5295084</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0051724</td>\n",
       "<td>1.0</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.637444           18.9306    18.9306            0.88             0.88                        0.189655        0.189655                   1793.06   1793.06\n",
       "    2        0.0200369                   0.44378            12.391     15.6608            0.576            0.728                       0.124138        0.313793                   1139.1    1466.08\n",
       "    3        0.0300553                   0.353321           9.63741    13.653             0.448            0.634667                    0.0965517       0.410345                   863.741   1265.3\n",
       "    4        0.0400737                   0.297087           7.74434    12.1758            0.36             0.566                       0.0775862       0.487931                   674.434   1117.58\n",
       "    5        0.050012                    0.255135           6.41892    11.0318            0.298387         0.512821                    0.0637931       0.551724                   541.892   1003.18\n",
       "    6        0.100024                    0.155488           3.58534    7.30859            0.166667         0.339744                    0.17931         0.731034                   258.534   630.859\n",
       "    7        0.150036                    0.108351           2.06847    5.56188            0.0961538        0.258547                    0.103448        0.834483                   106.847   456.188\n",
       "    8        0.200048                    0.0833297          0.861862   4.38688            0.0400641        0.203926                    0.0431034       0.877586                   -13.8138  338.688\n",
       "    9        0.299992                    0.0554018          0.517532   3.09778            0.0240577        0.144002                    0.0517241       0.92931                    -48.2468  209.778\n",
       "    10       0.400016                    0.0394951          0.31027    2.40077            0.0144231        0.111601                    0.0310345       0.960345                   -68.973   140.077\n",
       "    11       0.50004                     0.0284928          0.0689489  1.93433            0.00320513       0.0899183                   0.00689655      0.967241                   -93.1051  93.4328\n",
       "    12       0.600946                    0.02               0.170866   1.63822            0.00794281       0.0761536                   0.0172414       0.984483                   -82.9134  63.8222\n",
       "    13       0.700008                    0.0132973          0.0348092  1.41131            0.00161812       0.0656057                   0.00344828      0.987931                   -96.5191  41.1314\n",
       "    14       0.799952                    0.00822527         0.0517532  1.24145            0.00240577       0.0577096                   0.00517241      0.993103                   -94.8247  24.1454\n",
       "    15       0.900056                    0.00330124         0.0172234  1.1053             0.000800641      0.0513802                   0.00172414      0.994828                   -98.2777  10.5295\n",
       "    16       1                           0                  0.0517532  1                  0.00240577       0.0464855                   0.00517241      1                          -94.8247  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest (RF) - Final Model\n",
    "\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [100],\n",
    "                 'mtries' : [-1],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1],  \n",
    "                 'col_sample_rate_per_tree' : [.5], \n",
    "                 'min_split_improvement' : [1e-5] \n",
    "               }\n",
    "\n",
    "rf_grid = H2OGridSearch(H2ORandomForestEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                         grid_id='rf_grid_4',\n",
    "                         hyper_params=hyper_par)\n",
    "\n",
    "rf_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "rf_gridperf1 = rf_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(rf_gridperf1)\n",
    "\n",
    "# Grab the top RF model, chosen by validation F1\n",
    "best_rf = rf_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_rf_perf1 = best_rf.model_performance(test_h2o)\n",
    "print(best_rf_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rf_perf1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "      col_sample_rate_per_tree learn_rate max_depth min_split_improvement  \\\n",
      "0                          1.0        0.5        20                1.0E-5   \n",
      "1                          0.5        0.5        20                1.0E-5   \n",
      "2                          0.5        1.0        20                1.0E-5   \n",
      "3                          0.5        1.0        20                1.0E-5   \n",
      "4                          0.5        1.0        20                1.0E-4   \n",
      "5                          1.0        0.5        20                1.0E-4   \n",
      "6                          0.5        1.0        20                1.0E-4   \n",
      "7                          1.0        0.5        20                1.0E-5   \n",
      "8                          0.5        0.5        20                1.0E-5   \n",
      "9                          0.5        0.5        20                1.0E-5   \n",
      "10                         0.5        0.5        20                1.0E-5   \n",
      "11                         1.0        0.5        20                1.0E-4   \n",
      "12                         1.0        1.0        20                1.0E-5   \n",
      "13                         0.5        1.0        20                1.0E-4   \n",
      "14                         1.0        1.0        20                1.0E-4   \n",
      "15                         1.0        1.0        20                1.0E-4   \n",
      "16                         0.5        1.0        20                1.0E-4   \n",
      "17                         1.0        1.0        20                1.0E-5   \n",
      "18                         1.0        0.5        20                1.0E-4   \n",
      "19                         0.5        0.5        20                1.0E-4   \n",
      "20                         0.5        0.5        20                1.0E-4   \n",
      "21                         1.0        0.5        20                1.0E-4   \n",
      "22                         1.0        0.5        20                1.0E-5   \n",
      "23                         1.0        0.5        20                1.0E-5   \n",
      "24                         1.0        1.0        20                1.0E-5   \n",
      "25                         1.0        1.0        20                1.0E-4   \n",
      "26                         0.5        1.0        20                1.0E-5   \n",
      "27                         0.5        0.5        20                1.0E-4   \n",
      "28                         1.0        1.0        20                1.0E-4   \n",
      "29                         0.5        0.5        20                1.0E-4   \n",
      ".. ..                      ...        ...       ...                   ...   \n",
      "34                         1.0        1.0         2                1.0E-4   \n",
      "35                         1.0        1.0         2                1.0E-5   \n",
      "36                         0.5        1.0         2                1.0E-5   \n",
      "37                         0.5        1.0         2                1.0E-4   \n",
      "38                         0.5        1.0         2                1.0E-5   \n",
      "39                         0.5        1.0         2                1.0E-4   \n",
      "40                         1.0        0.5         2                1.0E-4   \n",
      "41                         1.0        0.5         2                1.0E-5   \n",
      "42                         1.0        0.5         2                1.0E-5   \n",
      "43                         1.0        0.5         2                1.0E-4   \n",
      "44                         0.5        0.5         2                1.0E-4   \n",
      "45                         0.5        0.5         2                1.0E-5   \n",
      "46                         0.5        0.5         2                1.0E-5   \n",
      "47                         0.5        0.5         2                1.0E-4   \n",
      "48                         1.0        1.0         2                1.0E-5   \n",
      "49                         1.0        1.0         2                1.0E-4   \n",
      "50                         1.0        1.0         2                1.0E-4   \n",
      "51                         1.0        1.0         2                1.0E-5   \n",
      "52                         0.5        1.0         2                1.0E-5   \n",
      "53                         0.5        1.0         2                1.0E-4   \n",
      "54                         0.5        1.0         2                1.0E-5   \n",
      "55                         0.5        1.0         2                1.0E-4   \n",
      "56                         1.0        0.5         2                1.0E-4   \n",
      "57                         1.0        0.5         2                1.0E-5   \n",
      "58                         1.0        0.5         2                1.0E-4   \n",
      "59                         1.0        0.5         2                1.0E-5   \n",
      "60                         0.5        0.5         2                1.0E-4   \n",
      "61                         0.5        0.5         2                1.0E-5   \n",
      "62                         0.5        0.5         2                1.0E-4   \n",
      "63                         0.5        0.5         2                1.0E-5   \n",
      "\n",
      "   ntrees   sample_rate          model_ids                  f1  \n",
      "0      50           1.0  gbm_grid_model_45  0.9996842836395783  \n",
      "1     100           1.0  gbm_grid_model_60  0.9996211642884202  \n",
      "2      50           1.0  gbm_grid_model_46  0.9996211642884202  \n",
      "3     100           1.0  gbm_grid_model_62    0.99958960760173  \n",
      "4     100  0.6320000291  gbm_grid_model_22  0.9994949494949494  \n",
      "5      50           1.0  gbm_grid_model_37  0.9994003093141433  \n",
      "6      50  0.6320000291   gbm_grid_model_6  0.9994003093141433  \n",
      "7     100           1.0  gbm_grid_model_61  0.9993687665698776  \n",
      "8      50           1.0  gbm_grid_model_44  0.9993372258166324  \n",
      "9     100  0.6320000291  gbm_grid_model_28  0.9993372258166324  \n",
      "10     50  0.6320000291  gbm_grid_model_12  0.9993370584335638  \n",
      "11    100           1.0  gbm_grid_model_53  0.9993056870542195  \n",
      "12    100           1.0  gbm_grid_model_63  0.9993056870542195  \n",
      "13     50           1.0  gbm_grid_model_38  0.9992741502824503  \n",
      "14     50           1.0  gbm_grid_model_39  0.9992741502824503  \n",
      "15    100           1.0  gbm_grid_model_55  0.9992741502824503  \n",
      "16    100           1.0  gbm_grid_model_54  0.9992110329157068  \n",
      "17     50           1.0  gbm_grid_model_47  0.9991795519091196  \n",
      "18    100  0.6320000291  gbm_grid_model_21  0.9990219277488562  \n",
      "19     50           1.0  gbm_grid_model_36  0.9989899627548765  \n",
      "20    100           1.0  gbm_grid_model_52  0.9989273771215849  \n",
      "21     50  0.6320000291   gbm_grid_model_5  0.9989271063426949  \n",
      "22     50  0.6320000291  gbm_grid_model_13  0.9989264287969688  \n",
      "23    100  0.6320000291  gbm_grid_model_29  0.9988324760973147  \n",
      "24     50  0.6320000291  gbm_grid_model_15  0.9960146761133604  \n",
      "25     50  0.6320000291   gbm_grid_model_7  0.9936346074674605  \n",
      "26    100  0.6320000291  gbm_grid_model_30  0.9896356783919599  \n",
      "27    100  0.6320000291  gbm_grid_model_20  0.9869028549891642  \n",
      "28    100  0.6320000291  gbm_grid_model_23  0.9785806941500301  \n",
      "29     50  0.6320000291   gbm_grid_model_4  0.9704499237417387  \n",
      "..    ...           ...                ...                 ...  \n",
      "34    100  0.6320000291  gbm_grid_model_19  0.9347892393094615  \n",
      "35    100  0.6320000291  gbm_grid_model_27  0.9347892393094615  \n",
      "36    100           1.0  gbm_grid_model_58  0.9329277566539924  \n",
      "37    100           1.0  gbm_grid_model_50  0.9329277566539924  \n",
      "38    100  0.6320000291  gbm_grid_model_26  0.9322862555766057  \n",
      "39    100  0.6320000291  gbm_grid_model_18  0.9322862555766057  \n",
      "40    100           1.0  gbm_grid_model_49  0.9180448139883226  \n",
      "41    100           1.0  gbm_grid_model_57  0.9180448139883226  \n",
      "42    100  0.6320000291  gbm_grid_model_25  0.9164045081090987  \n",
      "43    100  0.6320000291  gbm_grid_model_17  0.9164045081090987  \n",
      "44    100  0.6320000291  gbm_grid_model_16   0.913948774645292  \n",
      "45    100  0.6320000291  gbm_grid_model_24   0.913948774645292  \n",
      "46    100           1.0  gbm_grid_model_56  0.9129123584441161  \n",
      "47    100           1.0  gbm_grid_model_48  0.9129123584441161  \n",
      "48     50           1.0  gbm_grid_model_43  0.9104828133101535  \n",
      "49     50           1.0  gbm_grid_model_35  0.9104828133101535  \n",
      "50     50  0.6320000291   gbm_grid_model_3  0.9080505984597779  \n",
      "51     50  0.6320000291  gbm_grid_model_11  0.9080505984597779  \n",
      "52     50           1.0  gbm_grid_model_42  0.9072164948453609  \n",
      "53     50           1.0  gbm_grid_model_34  0.9072164948453609  \n",
      "54     50  0.6320000291  gbm_grid_model_10  0.9058217101273499  \n",
      "55     50  0.6320000291   gbm_grid_model_2  0.9058217101273499  \n",
      "56     50           1.0  gbm_grid_model_33  0.8942528033242689  \n",
      "57     50           1.0  gbm_grid_model_41  0.8942528033242689  \n",
      "58     50  0.6320000291   gbm_grid_model_1  0.8933534650109388  \n",
      "59     50  0.6320000291   gbm_grid_model_9  0.8933534650109388  \n",
      "60     50  0.6320000291   gbm_grid_model_0  0.8911570996035834  \n",
      "61     50  0.6320000291   gbm_grid_model_8  0.8911570996035834  \n",
      "62     50           1.0  gbm_grid_model_32  0.8908318606021146  \n",
      "63     50           1.0  gbm_grid_model_40  0.8908318606021146  \n",
      "\n",
      "[64 rows x 9 columns]\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0344008579332\n",
      "RMSE: 0.185474682729\n",
      "LogLoss: 0.393357642883\n",
      "Mean Per-Class Error: 0.168924721677\n",
      "AUC: 0.844708097956\n",
      "Gini: 0.689416195911\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.0143677282501: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11724.0</td>\n",
       "<td>173.0</td>\n",
       "<td>0.0145</td>\n",
       "<td> (173.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>297.0</td>\n",
       "<td>283.0</td>\n",
       "<td>0.5121</td>\n",
       "<td> (297.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12021.0</td>\n",
       "<td>456.0</td>\n",
       "<td>0.0377</td>\n",
       "<td> (470.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11724  173  0.0145   (173.0/11897.0)\n",
       "1      297    283  0.5121   (297.0/580.0)\n",
       "Total  12021  456  0.0377   (470.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0143677</td>\n",
       "<td>0.5463320</td>\n",
       "<td>271.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0000218</td>\n",
       "<td>0.6002495</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3728591</td>\n",
       "<td>0.6086519</td>\n",
       "<td>169.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9969515</td>\n",
       "<td>0.9640939</td>\n",
       "<td>34.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999994</td>\n",
       "<td>0.8692308</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999994</td>\n",
       "<td>0.9985711</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0565449</td>\n",
       "<td>0.5322465</td>\n",
       "<td>231.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0000020</td>\n",
       "<td>0.7293103</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0000020</td>\n",
       "<td>0.8310753</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.0143677    0.546332  271\n",
       "max f2                       2.18258e-05  0.600249  394\n",
       "max f0point5                 0.372859     0.608652  169\n",
       "max accuracy                 0.996951     0.964094  34\n",
       "max precision                0.999999     0.869231  0\n",
       "max recall                   2.79217e-08  1         399\n",
       "max specificity              0.999999     0.998571  0\n",
       "max absolute_mcc             0.0565449    0.532246  231\n",
       "max min_per_class_accuracy   2.02761e-06  0.72931   398\n",
       "max mean_per_class_accuracy  2.02761e-06  0.831075  398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999957</td>\n",
       "<td>18.5864276</td>\n",
       "<td>18.5864276</td>\n",
       "<td>0.864</td>\n",
       "<td>0.864</td>\n",
       "<td>0.1862069</td>\n",
       "<td>0.1862069</td>\n",
       "<td>1758.6427586</td>\n",
       "<td>1758.6427586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9728571</td>\n",
       "<td>13.5956276</td>\n",
       "<td>16.0910276</td>\n",
       "<td>0.632</td>\n",
       "<td>0.748</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3224138</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1509.1027586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.2014661</td>\n",
       "<td>10.6699862</td>\n",
       "<td>14.2840138</td>\n",
       "<td>0.496</td>\n",
       "<td>0.664</td>\n",
       "<td>0.1068966</td>\n",
       "<td>0.4293103</td>\n",
       "<td>966.9986207</td>\n",
       "<td>1328.4013793</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.0047453</td>\n",
       "<td>7.5722483</td>\n",
       "<td>12.6060724</td>\n",
       "<td>0.352</td>\n",
       "<td>0.586</td>\n",
       "<td>0.0758621</td>\n",
       "<td>0.5051724</td>\n",
       "<td>657.2248276</td>\n",
       "<td>1160.6072414</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.0004688</td>\n",
       "<td>4.1636263</td>\n",
       "<td>10.9284068</td>\n",
       "<td>0.1935484</td>\n",
       "<td>0.5080128</td>\n",
       "<td>0.0413793</td>\n",
       "<td>0.5465517</td>\n",
       "<td>316.3626251</td>\n",
       "<td>992.8406830</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0000012</td>\n",
       "<td>3.7921916</td>\n",
       "<td>7.3602992</td>\n",
       "<td>0.1762821</td>\n",
       "<td>0.3421474</td>\n",
       "<td>0.1896552</td>\n",
       "<td>0.7362069</td>\n",
       "<td>279.2191645</td>\n",
       "<td>636.0299237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0000001</td>\n",
       "<td>1.9650448</td>\n",
       "<td>5.5618811</td>\n",
       "<td>0.0913462</td>\n",
       "<td>0.2585470</td>\n",
       "<td>0.0982759</td>\n",
       "<td>0.8344828</td>\n",
       "<td>96.5044761</td>\n",
       "<td>456.1881079</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.7929128</td>\n",
       "<td>4.3696390</td>\n",
       "<td>0.0368590</td>\n",
       "<td>0.203125</td>\n",
       "<td>0.0396552</td>\n",
       "<td>0.8741379</td>\n",
       "<td>-20.7087202</td>\n",
       "<td>336.9639009</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.5175317</td>\n",
       "<td>3.0862893</td>\n",
       "<td>0.0240577</td>\n",
       "<td>0.1434678</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.9258621</td>\n",
       "<td>-48.2468269</td>\n",
       "<td>208.6289349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.2240841</td>\n",
       "<td>2.3705947</td>\n",
       "<td>0.0104167</td>\n",
       "<td>0.1101984</td>\n",
       "<td>0.0224138</td>\n",
       "<td>0.9482759</td>\n",
       "<td>-77.5915948</td>\n",
       "<td>137.0594657</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1896096</td>\n",
       "<td>1.9343277</td>\n",
       "<td>0.0088141</td>\n",
       "<td>0.0899183</td>\n",
       "<td>0.0189655</td>\n",
       "<td>0.9672414</td>\n",
       "<td>-81.0390418</td>\n",
       "<td>93.4327727</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.6207330</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0753406</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9724138</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>62.0732954</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1034234</td>\n",
       "<td>1.4039248</td>\n",
       "<td>0.0048077</td>\n",
       "<td>0.0652622</td>\n",
       "<td>0.0103448</td>\n",
       "<td>0.9827586</td>\n",
       "<td>-89.6576592</td>\n",
       "<td>40.3924812</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2349880</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0574091</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>23.4988029</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.1053935</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0513848</td>\n",
       "<td>0.0068966</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>10.5393516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517117</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0024038</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0051724</td>\n",
       "<td>1.0</td>\n",
       "<td>-94.8288296</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.999996           18.5864    18.5864            0.864            0.864                       0.186207        0.186207                   1758.64   1758.64\n",
       "    2        0.0200369                   0.972857           13.5956    16.091             0.632            0.748                       0.136207        0.322414                   1259.56   1509.1\n",
       "    3        0.0300553                   0.201466           10.67      14.284             0.496            0.664                       0.106897        0.42931                    966.999   1328.4\n",
       "    4        0.0400737                   0.00474527         7.57225    12.6061            0.352            0.586                       0.0758621       0.505172                   657.225   1160.61\n",
       "    5        0.050012                    0.000468822        4.16363    10.9284            0.193548         0.508013                    0.0413793       0.546552                   316.363   992.841\n",
       "    6        0.100024                    1.24154e-06        3.79219    7.3603             0.176282         0.342147                    0.189655        0.736207                   279.219   636.03\n",
       "    7        0.150036                    8.75252e-08        1.96504    5.56188            0.0913462        0.258547                    0.0982759       0.834483                   96.5045   456.188\n",
       "    8        0.200048                    1.85542e-08        0.792913   4.36964            0.036859         0.203125                    0.0396552       0.874138                   -20.7087  336.964\n",
       "    9        0.299992                    3.17789e-09        0.517532   3.08629            0.0240577        0.143468                    0.0517241       0.925862                   -48.2468  208.629\n",
       "    10       0.400016                    1.25157e-09        0.224084   2.37059            0.0104167        0.110198                    0.0224138       0.948276                   -77.5916  137.059\n",
       "    11       0.50004                     7.31451e-10        0.18961    1.93433            0.0088141        0.0899183                   0.0189655       0.967241                   -81.039   93.4328\n",
       "    12       0.599984                    4.81451e-10        0.0517532  1.62073            0.00240577       0.0753406                   0.00517241      0.972414                   -94.8247  62.0733\n",
       "    13       0.700008                    3.39749e-10        0.103423   1.40392            0.00480769       0.0652622                   0.0103448       0.982759                   -89.6577  40.3925\n",
       "    14       0.799952                    2.36012e-10        0.0517532  1.23499            0.00240577       0.0574091                   0.00517241      0.987931                   -94.8247  23.4988\n",
       "    15       0.899976                    1.47938e-10        0.0689489  1.10539            0.00320513       0.0513848                   0.00689655      0.994828                   -93.1051  10.5394\n",
       "    16       1                           9.16154e-12        0.0517117  1                  0.00240385       0.0464855                   0.00517241      1                          -94.8288  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine (GBM)\n",
    "\n",
    "# GBM hyperparameters\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [50, 100],\n",
    "                 'learn_rate' : [0.5, 1.],\n",
    "                 'max_depth' : [2, 20],\n",
    "                 'sample_rate' : [0.6320000291, 1.],\n",
    "                 'col_sample_rate_per_tree' : [0.5, 1.],\n",
    "                 'min_split_improvement' : [1e-4, 1e-5] \n",
    "               }\n",
    "\n",
    "# Train and validate a cartesian grid of GBMs\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(seed=my_seed, \n",
    "                                                            nfolds=5,\n",
    "                                                            fold_assignment='Modulo'),\n",
    "                          grid_id='gbm_grid',\n",
    "                          hyper_params=hyper_par)\n",
    "gbm_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "gbm_gridperf1 = gbm_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# Grab the top GBM model, chosen by validation F1\n",
    "best_gbm = gbm_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_gbm_perf1 = best_gbm.model_performance(test_h2o)\n",
    "print(best_gbm_perf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "     col_sample_rate_per_tree learn_rate max_depth min_split_improvement  \\\n",
      "0                         1.0        0.5        20                1.0E-5   \n",
      "1                         0.5        1.0        20                1.0E-5   \n",
      "2                         0.5        0.5        20                1.0E-5   \n",
      "3                         0.5        0.5        20                1.0E-5   \n",
      "4                         0.5        1.0        20                1.0E-5   \n",
      "5                         0.5        1.0        20                1.0E-5   \n",
      "6                         1.0        0.5        20                1.0E-5   \n",
      "7                         1.0        0.5        20                1.0E-5   \n",
      "8                         0.5        0.5        20                1.0E-5   \n",
      "9                         1.0        1.0        20                1.0E-5   \n",
      "10                        1.0        1.0        20                1.0E-5   \n",
      "11                        1.0        1.0        20                1.0E-5   \n",
      "12                        0.5        0.5        20                1.0E-5   \n",
      "13                        0.5        1.0        20                1.0E-5   \n",
      "14                        1.0        1.0        20                1.0E-5   \n",
      "15                        1.0        0.5        20                1.0E-5   \n",
      "\n",
      "   ntrees sample_rate            model_ids                  f1  \n",
      "0      50         1.0   gbm_grid_1_model_5  0.9996842836395783  \n",
      "1      50         1.0   gbm_grid_1_model_6  0.9996211642884202  \n",
      "2     100         1.0   gbm_grid_1_model_8  0.9996211642884202  \n",
      "3     150         1.0  gbm_grid_1_model_12  0.9996211642884202  \n",
      "4     100         1.0  gbm_grid_1_model_10    0.99958960760173  \n",
      "5     150         1.0  gbm_grid_1_model_14    0.99958960760173  \n",
      "6     100         1.0   gbm_grid_1_model_9  0.9993687665698776  \n",
      "7     150         1.0  gbm_grid_1_model_13  0.9993687665698776  \n",
      "8      50         1.0   gbm_grid_1_model_4  0.9993372258166324  \n",
      "9     100         1.0  gbm_grid_1_model_11  0.9993056870542195  \n",
      "10     50         1.0   gbm_grid_1_model_7  0.9991795519091196  \n",
      "11    150         1.0  gbm_grid_1_model_15  0.9990534486022592  \n",
      "12      5         1.0   gbm_grid_1_model_0  0.9966003525560313  \n",
      "13      5         1.0   gbm_grid_1_model_2  0.9959998740117799  \n",
      "14      5         1.0   gbm_grid_1_model_3  0.9955586354616184  \n",
      "15      5         1.0   gbm_grid_1_model_1  0.9955018715988802  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0344008579332\n",
      "RMSE: 0.185474682729\n",
      "LogLoss: 0.393357642883\n",
      "Mean Per-Class Error: 0.168924721677\n",
      "AUC: 0.844708097956\n",
      "Gini: 0.689416195911\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.0143677282501: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11724.0</td>\n",
       "<td>173.0</td>\n",
       "<td>0.0145</td>\n",
       "<td> (173.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>297.0</td>\n",
       "<td>283.0</td>\n",
       "<td>0.5121</td>\n",
       "<td> (297.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12021.0</td>\n",
       "<td>456.0</td>\n",
       "<td>0.0377</td>\n",
       "<td> (470.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11724  173  0.0145   (173.0/11897.0)\n",
       "1      297    283  0.5121   (297.0/580.0)\n",
       "Total  12021  456  0.0377   (470.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0143677</td>\n",
       "<td>0.5463320</td>\n",
       "<td>271.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0000218</td>\n",
       "<td>0.6002495</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.3728591</td>\n",
       "<td>0.6086519</td>\n",
       "<td>169.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9969515</td>\n",
       "<td>0.9640939</td>\n",
       "<td>34.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999994</td>\n",
       "<td>0.8692308</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999994</td>\n",
       "<td>0.9985711</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0565449</td>\n",
       "<td>0.5322465</td>\n",
       "<td>231.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0000020</td>\n",
       "<td>0.7293103</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0000020</td>\n",
       "<td>0.8310753</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.0143677    0.546332  271\n",
       "max f2                       2.18258e-05  0.600249  394\n",
       "max f0point5                 0.372859     0.608652  169\n",
       "max accuracy                 0.996951     0.964094  34\n",
       "max precision                0.999999     0.869231  0\n",
       "max recall                   2.79217e-08  1         399\n",
       "max specificity              0.999999     0.998571  0\n",
       "max absolute_mcc             0.0565449    0.532246  231\n",
       "max min_per_class_accuracy   2.02761e-06  0.72931   398\n",
       "max mean_per_class_accuracy  2.02761e-06  0.831075  398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999957</td>\n",
       "<td>18.5864276</td>\n",
       "<td>18.5864276</td>\n",
       "<td>0.864</td>\n",
       "<td>0.864</td>\n",
       "<td>0.1862069</td>\n",
       "<td>0.1862069</td>\n",
       "<td>1758.6427586</td>\n",
       "<td>1758.6427586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9728571</td>\n",
       "<td>13.5956276</td>\n",
       "<td>16.0910276</td>\n",
       "<td>0.632</td>\n",
       "<td>0.748</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3224138</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1509.1027586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.2014661</td>\n",
       "<td>10.6699862</td>\n",
       "<td>14.2840138</td>\n",
       "<td>0.496</td>\n",
       "<td>0.664</td>\n",
       "<td>0.1068966</td>\n",
       "<td>0.4293103</td>\n",
       "<td>966.9986207</td>\n",
       "<td>1328.4013793</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.0047453</td>\n",
       "<td>7.5722483</td>\n",
       "<td>12.6060724</td>\n",
       "<td>0.352</td>\n",
       "<td>0.586</td>\n",
       "<td>0.0758621</td>\n",
       "<td>0.5051724</td>\n",
       "<td>657.2248276</td>\n",
       "<td>1160.6072414</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.0004688</td>\n",
       "<td>4.1636263</td>\n",
       "<td>10.9284068</td>\n",
       "<td>0.1935484</td>\n",
       "<td>0.5080128</td>\n",
       "<td>0.0413793</td>\n",
       "<td>0.5465517</td>\n",
       "<td>316.3626251</td>\n",
       "<td>992.8406830</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0000012</td>\n",
       "<td>3.7921916</td>\n",
       "<td>7.3602992</td>\n",
       "<td>0.1762821</td>\n",
       "<td>0.3421474</td>\n",
       "<td>0.1896552</td>\n",
       "<td>0.7362069</td>\n",
       "<td>279.2191645</td>\n",
       "<td>636.0299237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0000001</td>\n",
       "<td>1.9650448</td>\n",
       "<td>5.5618811</td>\n",
       "<td>0.0913462</td>\n",
       "<td>0.2585470</td>\n",
       "<td>0.0982759</td>\n",
       "<td>0.8344828</td>\n",
       "<td>96.5044761</td>\n",
       "<td>456.1881079</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.7929128</td>\n",
       "<td>4.3696390</td>\n",
       "<td>0.0368590</td>\n",
       "<td>0.203125</td>\n",
       "<td>0.0396552</td>\n",
       "<td>0.8741379</td>\n",
       "<td>-20.7087202</td>\n",
       "<td>336.9639009</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.5175317</td>\n",
       "<td>3.0862893</td>\n",
       "<td>0.0240577</td>\n",
       "<td>0.1434678</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.9258621</td>\n",
       "<td>-48.2468269</td>\n",
       "<td>208.6289349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.2240841</td>\n",
       "<td>2.3705947</td>\n",
       "<td>0.0104167</td>\n",
       "<td>0.1101984</td>\n",
       "<td>0.0224138</td>\n",
       "<td>0.9482759</td>\n",
       "<td>-77.5915948</td>\n",
       "<td>137.0594657</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1896096</td>\n",
       "<td>1.9343277</td>\n",
       "<td>0.0088141</td>\n",
       "<td>0.0899183</td>\n",
       "<td>0.0189655</td>\n",
       "<td>0.9672414</td>\n",
       "<td>-81.0390418</td>\n",
       "<td>93.4327727</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.6207330</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0753406</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9724138</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>62.0732954</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1034234</td>\n",
       "<td>1.4039248</td>\n",
       "<td>0.0048077</td>\n",
       "<td>0.0652622</td>\n",
       "<td>0.0103448</td>\n",
       "<td>0.9827586</td>\n",
       "<td>-89.6576592</td>\n",
       "<td>40.3924812</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2349880</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0574091</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>23.4988029</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.1053935</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0513848</td>\n",
       "<td>0.0068966</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>10.5393516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517117</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0024038</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0051724</td>\n",
       "<td>1.0</td>\n",
       "<td>-94.8288296</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.999996           18.5864    18.5864            0.864            0.864                       0.186207        0.186207                   1758.64   1758.64\n",
       "    2        0.0200369                   0.972857           13.5956    16.091             0.632            0.748                       0.136207        0.322414                   1259.56   1509.1\n",
       "    3        0.0300553                   0.201466           10.67      14.284             0.496            0.664                       0.106897        0.42931                    966.999   1328.4\n",
       "    4        0.0400737                   0.00474527         7.57225    12.6061            0.352            0.586                       0.0758621       0.505172                   657.225   1160.61\n",
       "    5        0.050012                    0.000468822        4.16363    10.9284            0.193548         0.508013                    0.0413793       0.546552                   316.363   992.841\n",
       "    6        0.100024                    1.24154e-06        3.79219    7.3603             0.176282         0.342147                    0.189655        0.736207                   279.219   636.03\n",
       "    7        0.150036                    8.75252e-08        1.96504    5.56188            0.0913462        0.258547                    0.0982759       0.834483                   96.5045   456.188\n",
       "    8        0.200048                    1.85542e-08        0.792913   4.36964            0.036859         0.203125                    0.0396552       0.874138                   -20.7087  336.964\n",
       "    9        0.299992                    3.17789e-09        0.517532   3.08629            0.0240577        0.143468                    0.0517241       0.925862                   -48.2468  208.629\n",
       "    10       0.400016                    1.25157e-09        0.224084   2.37059            0.0104167        0.110198                    0.0224138       0.948276                   -77.5916  137.059\n",
       "    11       0.50004                     7.31451e-10        0.18961    1.93433            0.0088141        0.0899183                   0.0189655       0.967241                   -81.039   93.4328\n",
       "    12       0.599984                    4.81451e-10        0.0517532  1.62073            0.00240577       0.0753406                   0.00517241      0.972414                   -94.8247  62.0733\n",
       "    13       0.700008                    3.39749e-10        0.103423   1.40392            0.00480769       0.0652622                   0.0103448       0.982759                   -89.6577  40.3925\n",
       "    14       0.799952                    2.36012e-10        0.0517532  1.23499            0.00240577       0.0574091                   0.00517241      0.987931                   -94.8247  23.4988\n",
       "    15       0.899976                    1.47938e-10        0.0689489  1.10539            0.00320513       0.0513848                   0.00689655      0.994828                   -93.1051  10.5394\n",
       "    16       1                           9.16154e-12        0.0517117  1                  0.00240385       0.0464855                   0.00517241      1                          -94.8288  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine (GBM) - Refinement 1\n",
    "\n",
    "# GBM hyperparameters\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [5, 50, 100, 150],\n",
    "                 'learn_rate' : [0.5, 1.],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1.],\n",
    "                 'col_sample_rate_per_tree' : [0.5, 1.],\n",
    "                 'min_split_improvement' : [1e-5] \n",
    "               }\n",
    "                \n",
    "                 \n",
    "\n",
    "# Train and validate a cartesian grid of GBMs\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(seed=my_seed, \n",
    "                                                            nfolds=5,\n",
    "                                                            fold_assignment='Modulo'),\n",
    "                          grid_id='gbm_grid_1',\n",
    "                          hyper_params=hyper_par)\n",
    "\n",
    "gbm_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "gbm_gridperf1 = gbm_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# Grab the top GBM model, chosen by validation F1\n",
    "best_gbm = gbm_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_gbm_perf1 = best_gbm.model_performance(test_h2o)\n",
    "print(best_gbm_perf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    col_sample_rate_per_tree learn_rate max_depth min_split_improvement  \\\n",
      "0                        0.7        0.7        20                1.0E-5   \n",
      "1                        0.7        1.0        20                1.0E-5   \n",
      "2                        1.0        1.0        20                1.0E-5   \n",
      "3                        1.0        0.7        20                1.0E-5   \n",
      "\n",
      "  ntrees sample_rate           model_ids                  f1  \n",
      "0     50         1.0  gbm_grid_2_model_0  0.9995265002051832  \n",
      "1     50         1.0  gbm_grid_2_model_2  0.9994949494949494  \n",
      "2     50         1.0  gbm_grid_2_model_3  0.9991795519091196  \n",
      "3     50         1.0  gbm_grid_2_model_1  0.9991795519091196  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0353512396727\n",
      "RMSE: 0.188019253463\n",
      "LogLoss: 0.503581030709\n",
      "Mean Per-Class Error: 0.187013750207\n",
      "AUC: 0.820554877642\n",
      "Gini: 0.641109755285\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 1.7632596531e-05: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11581.0</td>\n",
       "<td>316.0</td>\n",
       "<td>0.0266</td>\n",
       "<td> (316.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>240.0</td>\n",
       "<td>340.0</td>\n",
       "<td>0.4138</td>\n",
       "<td> (240.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11821.0</td>\n",
       "<td>656.0</td>\n",
       "<td>0.0446</td>\n",
       "<td> (556.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11581  316  0.0266   (316.0/11897.0)\n",
       "1      240    340  0.4138   (240.0/580.0)\n",
       "Total  11821  656  0.0446   (556.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0000176</td>\n",
       "<td>0.5501618</td>\n",
       "<td>388.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0000006</td>\n",
       "<td>0.6001234</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7618122</td>\n",
       "<td>0.5967933</td>\n",
       "<td>123.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.8735708</td>\n",
       "<td>0.9636130</td>\n",
       "<td>109.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999978</td>\n",
       "<td>0.8549618</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999999</td>\n",
       "<td>0.9984030</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0000176</td>\n",
       "<td>0.5279184</td>\n",
       "<td>388.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0000006</td>\n",
       "<td>0.6706897</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0000006</td>\n",
       "<td>0.8129862</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       1.76326e-05  0.550162  388\n",
       "max f2                       6.46343e-07  0.600123  398\n",
       "max f0point5                 0.761812     0.596793  123\n",
       "max accuracy                 0.873571     0.963613  109\n",
       "max precision                0.999998     0.854962  1\n",
       "max recall                   2.77753e-09  1         399\n",
       "max specificity              1            0.998403  0\n",
       "max absolute_mcc             1.76326e-05  0.527918  388\n",
       "max min_per_class_accuracy   6.46343e-07  0.67069   398\n",
       "max mean_per_class_accuracy  6.46343e-07  0.812986  398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999989</td>\n",
       "<td>18.5864276</td>\n",
       "<td>18.5864276</td>\n",
       "<td>0.864</td>\n",
       "<td>0.864</td>\n",
       "<td>0.1862069</td>\n",
       "<td>0.1862069</td>\n",
       "<td>1758.6427586</td>\n",
       "<td>1758.6427586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9073908</td>\n",
       "<td>13.5956276</td>\n",
       "<td>16.0910276</td>\n",
       "<td>0.632</td>\n",
       "<td>0.748</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3224138</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1509.1027586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.0243313</td>\n",
       "<td>10.3257931</td>\n",
       "<td>14.1692828</td>\n",
       "<td>0.48</td>\n",
       "<td>0.6586667</td>\n",
       "<td>0.1034483</td>\n",
       "<td>0.4258621</td>\n",
       "<td>932.5793103</td>\n",
       "<td>1316.9282759</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.0005159</td>\n",
       "<td>6.8838621</td>\n",
       "<td>12.3479276</td>\n",
       "<td>0.32</td>\n",
       "<td>0.574</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.4948276</td>\n",
       "<td>588.3862069</td>\n",
       "<td>1134.7927586</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.0000251</td>\n",
       "<td>7.4598304</td>\n",
       "<td>11.3765749</td>\n",
       "<td>0.3467742</td>\n",
       "<td>0.5288462</td>\n",
       "<td>0.0741379</td>\n",
       "<td>0.5689655</td>\n",
       "<td>645.9830367</td>\n",
       "<td>1037.6574934</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0000000</td>\n",
       "<td>3.5163959</td>\n",
       "<td>7.4464854</td>\n",
       "<td>0.1634615</td>\n",
       "<td>0.3461538</td>\n",
       "<td>0.1758621</td>\n",
       "<td>0.7448276</td>\n",
       "<td>251.6395889</td>\n",
       "<td>644.6485411</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.7581979</td>\n",
       "<td>5.5503896</td>\n",
       "<td>0.0817308</td>\n",
       "<td>0.2580128</td>\n",
       "<td>0.0879310</td>\n",
       "<td>0.8327586</td>\n",
       "<td>75.8197944</td>\n",
       "<td>455.0389589</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.8618617</td>\n",
       "<td>4.3782576</td>\n",
       "<td>0.0400641</td>\n",
       "<td>0.2035256</td>\n",
       "<td>0.0431034</td>\n",
       "<td>0.8758621</td>\n",
       "<td>-13.8138263</td>\n",
       "<td>337.8257626</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.6900423</td>\n",
       "<td>3.1495094</td>\n",
       "<td>0.0320770</td>\n",
       "<td>0.1464066</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.9448276</td>\n",
       "<td>-30.9957692</td>\n",
       "<td>214.9509429</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1723723</td>\n",
       "<td>2.4050760</td>\n",
       "<td>0.0080128</td>\n",
       "<td>0.1118012</td>\n",
       "<td>0.0172414</td>\n",
       "<td>0.9620690</td>\n",
       "<td>-82.7627653</td>\n",
       "<td>140.5076033</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1551351</td>\n",
       "<td>1.9550157</td>\n",
       "<td>0.0072115</td>\n",
       "<td>0.0908799</td>\n",
       "<td>0.0155172</td>\n",
       "<td>0.9775862</td>\n",
       "<td>-84.4864887</td>\n",
       "<td>95.5015724</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0862553</td>\n",
       "<td>1.6437221</td>\n",
       "<td>0.0040096</td>\n",
       "<td>0.0764093</td>\n",
       "<td>0.0086207</td>\n",
       "<td>0.9862069</td>\n",
       "<td>-91.3744711</td>\n",
       "<td>64.3722074</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.4137769</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0657202</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9896552</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>41.3776916</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0345021</td>\n",
       "<td>1.2414539</td>\n",
       "<td>0.0016038</td>\n",
       "<td>0.0577096</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-96.5497885</td>\n",
       "<td>24.1453935</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1034778</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0512958</td>\n",
       "<td>0.0</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-100.0</td>\n",
       "<td>10.3477756</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0068966</td>\n",
       "<td>1.0</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.999999           18.5864    18.5864            0.864            0.864                       0.186207        0.186207                   1758.64   1758.64\n",
       "    2        0.0200369                   0.907391           13.5956    16.091             0.632            0.748                       0.136207        0.322414                   1259.56   1509.1\n",
       "    3        0.0300553                   0.0243313          10.3258    14.1693            0.48             0.658667                    0.103448        0.425862                   932.579   1316.93\n",
       "    4        0.0400737                   0.000515941        6.88386    12.3479            0.32             0.574                       0.0689655       0.494828                   588.386   1134.79\n",
       "    5        0.050012                    2.50965e-05        7.45983    11.3766            0.346774         0.528846                    0.0741379       0.568966                   645.983   1037.66\n",
       "    6        0.100024                    1.48014e-08        3.5164     7.44649            0.163462         0.346154                    0.175862        0.744828                   251.64    644.649\n",
       "    7        0.150036                    5.92876e-10        1.7582     5.55039            0.0817308        0.258013                    0.087931        0.832759                   75.8198   455.039\n",
       "    8        0.200048                    6.99234e-11        0.861862   4.37826            0.0400641        0.203526                    0.0431034       0.875862                   -13.8138  337.826\n",
       "    9        0.299992                    7.23811e-12        0.690042   3.14951            0.032077         0.146407                    0.0689655       0.944828                   -30.9958  214.951\n",
       "    10       0.400016                    2.02423e-12        0.172372   2.40508            0.00801282       0.111801                    0.0172414       0.962069                   -82.7628  140.508\n",
       "    11       0.50004                     9.03035e-13        0.155135   1.95502            0.00721154       0.0908799                   0.0155172       0.977586                   -84.4865  95.5016\n",
       "    12       0.599984                    4.72255e-13        0.0862553  1.64372            0.00400962       0.0764093                   0.00862069      0.986207                   -91.3745  64.3722\n",
       "    13       0.700008                    2.55591e-13        0.0344745  1.41378            0.00160256       0.0657202                   0.00344828      0.989655                   -96.5526  41.3777\n",
       "    14       0.799952                    1.29314e-13        0.0345021  1.24145            0.00160385       0.0577096                   0.00344828      0.993103                   -96.5498  24.1454\n",
       "    15       0.899976                    5.58048e-14        0          1.10348            0                0.0512958                   0               0.993103                   -100      10.3478\n",
       "    16       1                           6.21054e-16        0.0689489  1                  0.00320513       0.0464855                   0.00689655      1                          -93.1051  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine (GBM) - Refinement 2\n",
    "\n",
    "# GBM hyperparameters\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [50],\n",
    "                 'learn_rate' : [0.7, 1.],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1.],\n",
    "                 'col_sample_rate_per_tree' : [0.7, 1.],\n",
    "                 'min_split_improvement' : [1e-5] \n",
    "               }\n",
    "                \n",
    "                 \n",
    "\n",
    "# Train and validate a cartesian grid of GBMs\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(seed=my_seed, \n",
    "                                                            nfolds=5,\n",
    "                                                            fold_assignment='Modulo'),\n",
    "                          grid_id='gbm_grid_2',\n",
    "                          hyper_params=hyper_par)\n",
    "\n",
    "gbm_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "gbm_gridperf1 = gbm_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# Grab the top GBM model, chosen by validation F1\n",
    "best_gbm = gbm_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_gbm_perf1 = best_gbm.model_performance(test_h2o)\n",
    "print(best_gbm_perf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    col_sample_rate_per_tree learn_rate max_depth min_split_improvement  \\\n",
      "0                        0.7        0.9        20                1.0E-5   \n",
      "1                        0.7        0.7        20                1.0E-5   \n",
      "\n",
      "  ntrees sample_rate           model_ids                  f1  \n",
      "0     50         1.0  gbm_grid_3_model_1  0.9996211642884202  \n",
      "1     50         1.0  gbm_grid_3_model_0  0.9995265002051832  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0356139622167\n",
      "RMSE: 0.188716618814\n",
      "LogLoss: 0.631079580147\n",
      "Mean Per-Class Error: 0.199800224919\n",
      "AUC: 0.805697394011\n",
      "Gini: 0.611394788022\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 5.47646633027e-07: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11583.0</td>\n",
       "<td>314.0</td>\n",
       "<td>0.0264</td>\n",
       "<td> (314.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>237.0</td>\n",
       "<td>343.0</td>\n",
       "<td>0.4086</td>\n",
       "<td> (237.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11820.0</td>\n",
       "<td>657.0</td>\n",
       "<td>0.0442</td>\n",
       "<td> (551.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11583  314  0.0264   (314.0/11897.0)\n",
       "1      237    343  0.4086   (237.0/580.0)\n",
       "Total  11820  657  0.0442   (551.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0000005</td>\n",
       "<td>0.5545675</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.5922953</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.1893813</td>\n",
       "<td>0.5983146</td>\n",
       "<td>159.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9608633</td>\n",
       "<td>0.9637733</td>\n",
       "<td>99.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.8677686</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.9986551</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0000005</td>\n",
       "<td>0.5325729</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.6362069</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.8001998</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       5.47647e-07  0.554568  393\n",
       "max f2                       5.99568e-08  0.592295  398\n",
       "max f0point5                 0.189381     0.598315  159\n",
       "max accuracy                 0.960863     0.963773  99\n",
       "max precision                1            0.867769  0\n",
       "max recall                   2.52511e-10  1         399\n",
       "max specificity              1            0.998655  0\n",
       "max absolute_mcc             5.47647e-07  0.532573  393\n",
       "max min_per_class_accuracy   5.99568e-08  0.636207  398\n",
       "max mean_per_class_accuracy  5.99568e-08  0.8002    398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999997</td>\n",
       "<td>18.4143310</td>\n",
       "<td>18.4143310</td>\n",
       "<td>0.856</td>\n",
       "<td>0.856</td>\n",
       "<td>0.1844828</td>\n",
       "<td>0.1844828</td>\n",
       "<td>1741.4331035</td>\n",
       "<td>1741.4331035</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9155730</td>\n",
       "<td>13.5956276</td>\n",
       "<td>16.0049793</td>\n",
       "<td>0.632</td>\n",
       "<td>0.744</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3206897</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1500.4979310</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.0042370</td>\n",
       "<td>10.1536966</td>\n",
       "<td>14.0545517</td>\n",
       "<td>0.472</td>\n",
       "<td>0.6533333</td>\n",
       "<td>0.1017241</td>\n",
       "<td>0.4224138</td>\n",
       "<td>915.3696552</td>\n",
       "<td>1305.4551724</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.0000405</td>\n",
       "<td>7.9164414</td>\n",
       "<td>12.5200241</td>\n",
       "<td>0.368</td>\n",
       "<td>0.582</td>\n",
       "<td>0.0793103</td>\n",
       "<td>0.5017241</td>\n",
       "<td>691.6441379</td>\n",
       "<td>1152.0024138</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.0000010</td>\n",
       "<td>6.7658927</td>\n",
       "<td>11.3765749</td>\n",
       "<td>0.3145161</td>\n",
       "<td>0.5288462</td>\n",
       "<td>0.0672414</td>\n",
       "<td>0.5689655</td>\n",
       "<td>576.5892659</td>\n",
       "<td>1037.6574934</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0000000</td>\n",
       "<td>3.4474469</td>\n",
       "<td>7.4120109</td>\n",
       "<td>0.1602564</td>\n",
       "<td>0.3445513</td>\n",
       "<td>0.1724138</td>\n",
       "<td>0.7413793</td>\n",
       "<td>244.7446950</td>\n",
       "<td>641.2010942</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.7926724</td>\n",
       "<td>5.5388981</td>\n",
       "<td>0.0833333</td>\n",
       "<td>0.2574786</td>\n",
       "<td>0.0896552</td>\n",
       "<td>0.8310345</td>\n",
       "<td>79.2672414</td>\n",
       "<td>453.8898099</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.8963362</td>\n",
       "<td>4.3782576</td>\n",
       "<td>0.0416667</td>\n",
       "<td>0.2035256</td>\n",
       "<td>0.0448276</td>\n",
       "<td>0.8758621</td>\n",
       "<td>-10.3663793</td>\n",
       "<td>337.8257626</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.4657786</td>\n",
       "<td>3.0747948</td>\n",
       "<td>0.0216520</td>\n",
       "<td>0.1429335</td>\n",
       "<td>0.0465517</td>\n",
       "<td>0.9224138</td>\n",
       "<td>-53.4221442</td>\n",
       "<td>207.4794789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.2585585</td>\n",
       "<td>2.3705947</td>\n",
       "<td>0.0120192</td>\n",
       "<td>0.1101984</td>\n",
       "<td>0.0258621</td>\n",
       "<td>0.9482759</td>\n",
       "<td>-74.1441479</td>\n",
       "<td>137.0594657</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1034234</td>\n",
       "<td>1.9170877</td>\n",
       "<td>0.0048077</td>\n",
       "<td>0.0891168</td>\n",
       "<td>0.0103448</td>\n",
       "<td>0.9586207</td>\n",
       "<td>-89.6576592</td>\n",
       "<td>91.7087730</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1552595</td>\n",
       "<td>1.6236066</td>\n",
       "<td>0.0072173</td>\n",
       "<td>0.0754742</td>\n",
       "<td>0.0155172</td>\n",
       "<td>0.9741379</td>\n",
       "<td>-84.4740481</td>\n",
       "<td>62.3606594</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1378979</td>\n",
       "<td>1.4113139</td>\n",
       "<td>0.0064103</td>\n",
       "<td>0.0656057</td>\n",
       "<td>0.0137931</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-86.2102122</td>\n",
       "<td>41.1313890</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2414539</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0577096</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>24.1453935</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.1073093</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0514739</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9965517</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>10.7309276</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0034483</td>\n",
       "<td>1.0</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   1                  18.4143    18.4143            0.856            0.856                       0.184483        0.184483                   1741.43   1741.43\n",
       "    2        0.0200369                   0.915573           13.5956    16.005             0.632            0.744                       0.136207        0.32069                    1259.56   1500.5\n",
       "    3        0.0300553                   0.00423705         10.1537    14.0546            0.472            0.653333                    0.101724        0.422414                   915.37    1305.46\n",
       "    4        0.0400737                   4.05435e-05        7.91644    12.52              0.368            0.582                       0.0793103       0.501724                   691.644   1152\n",
       "    5        0.050012                    9.61238e-07        6.76589    11.3766            0.314516         0.528846                    0.0672414       0.568966                   576.589   1037.66\n",
       "    6        0.100024                    1.96826e-10        3.44745    7.41201            0.160256         0.344551                    0.172414        0.741379                   244.745   641.201\n",
       "    7        0.150036                    3.08306e-12        1.79267    5.5389             0.0833333        0.257479                    0.0896552       0.831034                   79.2672   453.89\n",
       "    8        0.200048                    2.93675e-13        0.896336   4.37826            0.0416667        0.203526                    0.0448276       0.875862                   -10.3664  337.826\n",
       "    9        0.299992                    1.96264e-14        0.465779   3.07479            0.021652         0.142933                    0.0465517       0.922414                   -53.4221  207.479\n",
       "    10       0.400016                    4.45871e-15        0.258559   2.37059            0.0120192        0.110198                    0.0258621       0.948276                   -74.1441  137.059\n",
       "    11       0.50004                     1.64567e-15        0.103423   1.91709            0.00480769       0.0891168                   0.0103448       0.958621                   -89.6577  91.7088\n",
       "    12       0.599984                    6.92172e-16        0.15526    1.62361            0.00721732       0.0754742                   0.0155172       0.974138                   -84.474   62.3607\n",
       "    13       0.700008                    3.03404e-16        0.137898   1.41131            0.00641026       0.0656057                   0.0137931       0.987931                   -86.2102  41.1314\n",
       "    14       0.799952                    1.20067e-16        0.0517532  1.24145            0.00240577       0.0577096                   0.00517241      0.993103                   -94.8247  24.1454\n",
       "    15       0.899976                    3.38794e-17        0.0344745  1.10731            0.00160256       0.0514739                   0.00344828      0.996552                   -96.5526  10.7309\n",
       "    16       1                           1e-19              0.0344745  1                  0.00160256       0.0464855                   0.00344828      1                          -96.5526  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine (GBM) - Refinement 3\n",
    "\n",
    "# GBM hyperparameters\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [50],\n",
    "                 'learn_rate' : [.7, .9],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1.],\n",
    "                 'col_sample_rate_per_tree' : [0.7],\n",
    "                 'min_split_improvement' : [1e-5] \n",
    "               }\n",
    "                \n",
    "                 \n",
    "\n",
    "# Train and validate a cartesian grid of GBMs\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(seed=my_seed, \n",
    "                                                            nfolds=5,\n",
    "                                                            fold_assignment='Modulo'),\n",
    "                          grid_id='gbm_grid_3',\n",
    "                          hyper_params=hyper_par)\n",
    "\n",
    "gbm_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "gbm_gridperf1 = gbm_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# Grab the top GBM model, chosen by validation F1\n",
    "best_gbm = gbm_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_gbm_perf1 = best_gbm.model_performance(test_h2o)\n",
    "print(best_gbm_perf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm Grid Build progress: |████████████████████████████████████████████████| 100%\n",
      "    col_sample_rate_per_tree learn_rate max_depth min_split_improvement  \\\n",
      "0                        0.7        0.9        20                1.0E-5   \n",
      "1                        0.7        0.8        20                1.0E-5   \n",
      "\n",
      "  ntrees sample_rate           model_ids                  f1  \n",
      "0     50         1.0  gbm_grid_4_model_1  0.9996211642884202  \n",
      "1     50         1.0  gbm_grid_4_model_0  0.9994318540496181  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0356139622167\n",
      "RMSE: 0.188716618814\n",
      "LogLoss: 0.631079580147\n",
      "Mean Per-Class Error: 0.199800224919\n",
      "AUC: 0.805697394011\n",
      "Gini: 0.611394788022\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 5.47646633027e-07: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11583.0</td>\n",
       "<td>314.0</td>\n",
       "<td>0.0264</td>\n",
       "<td> (314.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>237.0</td>\n",
       "<td>343.0</td>\n",
       "<td>0.4086</td>\n",
       "<td> (237.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11820.0</td>\n",
       "<td>657.0</td>\n",
       "<td>0.0442</td>\n",
       "<td> (551.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11583  314  0.0264   (314.0/11897.0)\n",
       "1      237    343  0.4086   (237.0/580.0)\n",
       "Total  11820  657  0.0442   (551.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.0000005</td>\n",
       "<td>0.5545675</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.5922953</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.1893813</td>\n",
       "<td>0.5983146</td>\n",
       "<td>159.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9608633</td>\n",
       "<td>0.9637733</td>\n",
       "<td>99.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.8677686</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>1.0000000</td>\n",
       "<td>0.9986551</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.0000005</td>\n",
       "<td>0.5325729</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.6362069</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0000001</td>\n",
       "<td>0.8001998</td>\n",
       "<td>398.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       5.47647e-07  0.554568  393\n",
       "max f2                       5.99568e-08  0.592295  398\n",
       "max f0point5                 0.189381     0.598315  159\n",
       "max accuracy                 0.960863     0.963773  99\n",
       "max precision                1            0.867769  0\n",
       "max recall                   2.52511e-10  1         399\n",
       "max specificity              1            0.998655  0\n",
       "max absolute_mcc             5.47647e-07  0.532573  393\n",
       "max min_per_class_accuracy   5.99568e-08  0.636207  398\n",
       "max mean_per_class_accuracy  5.99568e-08  0.8002    398"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999997</td>\n",
       "<td>18.4143310</td>\n",
       "<td>18.4143310</td>\n",
       "<td>0.856</td>\n",
       "<td>0.856</td>\n",
       "<td>0.1844828</td>\n",
       "<td>0.1844828</td>\n",
       "<td>1741.4331035</td>\n",
       "<td>1741.4331035</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9155730</td>\n",
       "<td>13.5956276</td>\n",
       "<td>16.0049793</td>\n",
       "<td>0.632</td>\n",
       "<td>0.744</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3206897</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1500.4979310</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.0042370</td>\n",
       "<td>10.1536966</td>\n",
       "<td>14.0545517</td>\n",
       "<td>0.472</td>\n",
       "<td>0.6533333</td>\n",
       "<td>0.1017241</td>\n",
       "<td>0.4224138</td>\n",
       "<td>915.3696552</td>\n",
       "<td>1305.4551724</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.0000405</td>\n",
       "<td>7.9164414</td>\n",
       "<td>12.5200241</td>\n",
       "<td>0.368</td>\n",
       "<td>0.582</td>\n",
       "<td>0.0793103</td>\n",
       "<td>0.5017241</td>\n",
       "<td>691.6441379</td>\n",
       "<td>1152.0024138</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.0000010</td>\n",
       "<td>6.7658927</td>\n",
       "<td>11.3765749</td>\n",
       "<td>0.3145161</td>\n",
       "<td>0.5288462</td>\n",
       "<td>0.0672414</td>\n",
       "<td>0.5689655</td>\n",
       "<td>576.5892659</td>\n",
       "<td>1037.6574934</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0000000</td>\n",
       "<td>3.4474469</td>\n",
       "<td>7.4120109</td>\n",
       "<td>0.1602564</td>\n",
       "<td>0.3445513</td>\n",
       "<td>0.1724138</td>\n",
       "<td>0.7413793</td>\n",
       "<td>244.7446950</td>\n",
       "<td>641.2010942</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0000000</td>\n",
       "<td>1.7926724</td>\n",
       "<td>5.5388981</td>\n",
       "<td>0.0833333</td>\n",
       "<td>0.2574786</td>\n",
       "<td>0.0896552</td>\n",
       "<td>0.8310345</td>\n",
       "<td>79.2672414</td>\n",
       "<td>453.8898099</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.8963362</td>\n",
       "<td>4.3782576</td>\n",
       "<td>0.0416667</td>\n",
       "<td>0.2035256</td>\n",
       "<td>0.0448276</td>\n",
       "<td>0.8758621</td>\n",
       "<td>-10.3663793</td>\n",
       "<td>337.8257626</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.4657786</td>\n",
       "<td>3.0747948</td>\n",
       "<td>0.0216520</td>\n",
       "<td>0.1429335</td>\n",
       "<td>0.0465517</td>\n",
       "<td>0.9224138</td>\n",
       "<td>-53.4221442</td>\n",
       "<td>207.4794789</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.2585585</td>\n",
       "<td>2.3705947</td>\n",
       "<td>0.0120192</td>\n",
       "<td>0.1101984</td>\n",
       "<td>0.0258621</td>\n",
       "<td>0.9482759</td>\n",
       "<td>-74.1441479</td>\n",
       "<td>137.0594657</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1034234</td>\n",
       "<td>1.9170877</td>\n",
       "<td>0.0048077</td>\n",
       "<td>0.0891168</td>\n",
       "<td>0.0103448</td>\n",
       "<td>0.9586207</td>\n",
       "<td>-89.6576592</td>\n",
       "<td>91.7087730</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1552595</td>\n",
       "<td>1.6236066</td>\n",
       "<td>0.0072173</td>\n",
       "<td>0.0754742</td>\n",
       "<td>0.0155172</td>\n",
       "<td>0.9741379</td>\n",
       "<td>-84.4740481</td>\n",
       "<td>62.3606594</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.1378979</td>\n",
       "<td>1.4113139</td>\n",
       "<td>0.0064103</td>\n",
       "<td>0.0656057</td>\n",
       "<td>0.0137931</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-86.2102122</td>\n",
       "<td>41.1313890</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2414539</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0577096</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9931034</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>24.1453935</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.1073093</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0514739</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9965517</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>10.7309276</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>1e-19</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0034483</td>\n",
       "<td>1.0</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   1                  18.4143    18.4143            0.856            0.856                       0.184483        0.184483                   1741.43   1741.43\n",
       "    2        0.0200369                   0.915573           13.5956    16.005             0.632            0.744                       0.136207        0.32069                    1259.56   1500.5\n",
       "    3        0.0300553                   0.00423705         10.1537    14.0546            0.472            0.653333                    0.101724        0.422414                   915.37    1305.46\n",
       "    4        0.0400737                   4.05435e-05        7.91644    12.52              0.368            0.582                       0.0793103       0.501724                   691.644   1152\n",
       "    5        0.050012                    9.61238e-07        6.76589    11.3766            0.314516         0.528846                    0.0672414       0.568966                   576.589   1037.66\n",
       "    6        0.100024                    1.96826e-10        3.44745    7.41201            0.160256         0.344551                    0.172414        0.741379                   244.745   641.201\n",
       "    7        0.150036                    3.08306e-12        1.79267    5.5389             0.0833333        0.257479                    0.0896552       0.831034                   79.2672   453.89\n",
       "    8        0.200048                    2.93675e-13        0.896336   4.37826            0.0416667        0.203526                    0.0448276       0.875862                   -10.3664  337.826\n",
       "    9        0.299992                    1.96264e-14        0.465779   3.07479            0.021652         0.142933                    0.0465517       0.922414                   -53.4221  207.479\n",
       "    10       0.400016                    4.45871e-15        0.258559   2.37059            0.0120192        0.110198                    0.0258621       0.948276                   -74.1441  137.059\n",
       "    11       0.50004                     1.64567e-15        0.103423   1.91709            0.00480769       0.0891168                   0.0103448       0.958621                   -89.6577  91.7088\n",
       "    12       0.599984                    6.92172e-16        0.15526    1.62361            0.00721732       0.0754742                   0.0155172       0.974138                   -84.474   62.3607\n",
       "    13       0.700008                    3.03404e-16        0.137898   1.41131            0.00641026       0.0656057                   0.0137931       0.987931                   -86.2102  41.1314\n",
       "    14       0.799952                    1.20067e-16        0.0517532  1.24145            0.00240577       0.0577096                   0.00517241      0.993103                   -94.8247  24.1454\n",
       "    15       0.899976                    3.38794e-17        0.0344745  1.10731            0.00160256       0.0514739                   0.00344828      0.996552                   -96.5526  10.7309\n",
       "    16       1                           1e-19              0.0344745  1                  0.00160256       0.0464855                   0.00344828      1                          -96.5526  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Machine (GBM) - Refinement 4\n",
    "\n",
    "# GBM hyperparameters\n",
    "hyper_par    = { \n",
    "                 'ntrees' :  [50],\n",
    "                 'learn_rate' : [.8, .9],\n",
    "                 'max_depth' : [20],\n",
    "                 'sample_rate' : [1.],\n",
    "                 'col_sample_rate_per_tree' : [0.7],\n",
    "                 'min_split_improvement' : [1e-5] \n",
    "               }\n",
    "                \n",
    "                 \n",
    "\n",
    "# Train and validate a cartesian grid of GBMs\n",
    "gbm_grid = H2OGridSearch(model=H2OGradientBoostingEstimator(seed=my_seed, \n",
    "                                                            nfolds=5,\n",
    "                                                            fold_assignment='Modulo'),\n",
    "                          grid_id='gbm_grid_4',\n",
    "                          hyper_params=hyper_par)\n",
    "\n",
    "gbm_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "gbm_gridperf1 = gbm_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(gbm_gridperf1)\n",
    "\n",
    "# Grab the top GBM model, chosen by validation F1\n",
    "best_gbm = gbm_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_gbm_perf1 = best_gbm.model_performance(test_h2o)\n",
    "print(best_gbm_perf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUiJQkREElKiEBGRhJQoREQkof8Py903AnWQepIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f755e298250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_gbm_perf1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "                  activation              epochs        hidden  \\\n",
      "0                       Tanh   5.197171810615108      [10, 10]   \n",
      "1                     Maxout  10.400028398673907  [10, 10, 10]   \n",
      "2       RectifierWithDropout   5.197171810615108      [10, 10]   \n",
      "3                     Maxout   5.197171810615108      [10, 10]   \n",
      "4       RectifierWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "5                  Rectifier   5.197171810615108  [10, 10, 10]   \n",
      "6                       Tanh  10.400028398673907  [10, 10, 10]   \n",
      "7                     Maxout  10.400028398673907      [10, 10]   \n",
      "8          MaxoutWithDropout   5.197171810615108      [10, 10]   \n",
      "9                  Rectifier   5.197171810615108  [10, 10, 10]   \n",
      "10      RectifierWithDropout  10.400028398673907      [10, 10]   \n",
      "11                      Tanh   5.197171810615108  [10, 10, 10]   \n",
      "12                    Maxout  10.400028398673907      [10, 10]   \n",
      "13                      Tanh  10.400028398673907      [10, 10]   \n",
      "14                    Maxout   5.197171810615108      [10, 10]   \n",
      "15                 Rectifier   5.197171810615108      [10, 10]   \n",
      "16                      Tanh  10.400028398673907  [10, 10, 10]   \n",
      "17                    Maxout  10.400028398673907  [10, 10, 10]   \n",
      "18           TanhWithDropout   5.197171810615108      [10, 10]   \n",
      "19                    Maxout   5.197171810615108  [10, 10, 10]   \n",
      "20      RectifierWithDropout   5.197171810615108      [10, 10]   \n",
      "21         MaxoutWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "22           TanhWithDropout  10.400028398673907  [10, 10, 10]   \n",
      "23                      Tanh   5.197171810615108      [10, 10]   \n",
      "24         MaxoutWithDropout   5.197171810615108      [10, 10]   \n",
      "25                    Maxout   5.197171810615108  [10, 10, 10]   \n",
      "26                      Tanh   5.197171810615108  [10, 10, 10]   \n",
      "27           TanhWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "28                 Rectifier  10.400028398673907  [10, 10, 10]   \n",
      "29         MaxoutWithDropout  10.400028398673907      [10, 10]   \n",
      "..  ..                   ...                 ...           ...   \n",
      "162                Rectifier   5.197171810615108  [10, 10, 10]   \n",
      "163          TanhWithDropout   5.197171810615108      [10, 10]   \n",
      "164                Rectifier   5.197171810615108  [10, 10, 10]   \n",
      "165     RectifierWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "166     RectifierWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "167          TanhWithDropout   5.197171810615108      [10, 10]   \n",
      "168        MaxoutWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "169                   Maxout  10.400028398673907      [10, 10]   \n",
      "170                     Tanh   5.197171810615108      [10, 10]   \n",
      "171                   Maxout   5.197171810615108      [10, 10]   \n",
      "172          TanhWithDropout   5.197171810615108  [10, 10, 10]   \n",
      "173                   Maxout  10.400028398673907  [10, 10, 10]   \n",
      "174        MaxoutWithDropout  10.400028398673907      [10, 10]   \n",
      "175                     Tanh  10.400028398673907  [10, 10, 10]   \n",
      "176                Rectifier   5.197171810615108  [10, 10, 10]   \n",
      "177                   Maxout   5.197171810615108  [10, 10, 10]   \n",
      "178                   Maxout  10.400028398673907  [10, 10, 10]   \n",
      "179          TanhWithDropout  10.400028398673907      [10, 10]   \n",
      "180                     Tanh  10.400028398673907      [10, 10]   \n",
      "181                Rectifier  10.400028398673907  [10, 10, 10]   \n",
      "182                Rectifier  10.400028398673907      [10, 10]   \n",
      "183                     Tanh  10.400028398673907      [10, 10]   \n",
      "184                Rectifier   5.197171810615108      [10, 10]   \n",
      "185        MaxoutWithDropout  10.400028398673907      [10, 10]   \n",
      "186        MaxoutWithDropout  10.400028398673907  [10, 10, 10]   \n",
      "187                Rectifier   5.197171810615108      [10, 10]   \n",
      "188     RectifierWithDropout  10.400028398673907  [10, 10, 10]   \n",
      "189                Rectifier  10.400028398673907      [10, 10]   \n",
      "190                     Tanh  10.400028398673907      [10, 10]   \n",
      "191                Rectifier  10.400028398673907  [10, 10, 10]   \n",
      "\n",
      "    input_dropout_ratio   l1   l2          model_ids                  f1  \n",
      "0                   0.1  0.0  0.0    dl_grid_model_0   0.826980728051392  \n",
      "1                   0.1  0.0  0.0   dl_grid_model_22  0.8218824829618714  \n",
      "2                   0.2  0.0  0.0   dl_grid_model_27  0.8186171711164669  \n",
      "3                   0.1  0.0  0.0    dl_grid_model_4   0.817534503472324  \n",
      "4                   0.2  0.0  0.0   dl_grid_model_39  0.8140697199258164  \n",
      "5                   0.2  0.0  0.0   dl_grid_model_38  0.8114303367315416  \n",
      "6                   0.2  0.0  0.0   dl_grid_model_42   0.806821324320172  \n",
      "7                   0.2  0.0  0.0   dl_grid_model_34  0.8067000626174078  \n",
      "8                   0.2  0.0  0.0   dl_grid_model_29  0.8053516170135308  \n",
      "9                   0.1  0.0  0.0   dl_grid_model_14  0.8011743324479239  \n",
      "10                  0.1  0.0  0.0    dl_grid_model_9  0.7999615125565285  \n",
      "11                  0.1  0.0  0.0   dl_grid_model_12  0.7975592703235238  \n",
      "12                  0.1  0.0  0.0   dl_grid_model_10  0.7969810880904509  \n",
      "13                  0.1  0.0  0.0    dl_grid_model_6  0.7964301426685941  \n",
      "14                  0.2  0.0  0.0   dl_grid_model_28  0.7954193372011785  \n",
      "15                  0.2  0.0  0.0   dl_grid_model_26  0.7929203539823009  \n",
      "16                  0.1  0.0  0.0   dl_grid_model_18   0.789810080350621  \n",
      "17                  0.2  0.0  0.0   dl_grid_model_46  0.7882909520993495  \n",
      "18                  0.2  0.0  0.0   dl_grid_model_25  0.7877222608522896  \n",
      "19                  0.1  0.0  0.0   dl_grid_model_16  0.7875953419755799  \n",
      "20                  0.1  0.0  0.0    dl_grid_model_3  0.7873199973936273  \n",
      "21                  0.1  0.0  0.0   dl_grid_model_17  0.7868455319020077  \n",
      "22                  0.2  0.0  0.0   dl_grid_model_43  0.7863601986123443  \n",
      "23                  0.2  0.0  0.0   dl_grid_model_24   0.785901782784644  \n",
      "24                  0.1  0.0  0.0    dl_grid_model_5  0.7852351012304082  \n",
      "25                  0.2  0.0  0.0   dl_grid_model_40  0.7844522968197879  \n",
      "26                  0.2  0.0  0.0   dl_grid_model_36  0.7830171849913778  \n",
      "27                  0.2  0.0  0.0   dl_grid_model_37  0.7826610073192741  \n",
      "28                  0.1  0.0  0.0   dl_grid_model_20  0.7786493311126632  \n",
      "29                  0.2  0.0  0.0   dl_grid_model_35  0.7785707607442267  \n",
      "..                  ...  ...  ...                ...                 ...  \n",
      "162                 0.2  0.0  0.5  dl_grid_model_134  0.6666666666666666  \n",
      "163                 0.1  0.5  0.5  dl_grid_model_145  0.6666666666666666  \n",
      "164                 0.2  0.5  0.5  dl_grid_model_182  0.6666666666666666  \n",
      "165                 0.1  0.5  0.5  dl_grid_model_159  0.6666666666666666  \n",
      "166                 0.2  0.5  0.0   dl_grid_model_87  0.6666666666666666  \n",
      "167                 0.2  0.5  0.0   dl_grid_model_73  0.6666666666666666  \n",
      "168                 0.2  0.5  0.0   dl_grid_model_89  0.6666666666666666  \n",
      "169                 0.2  0.0  0.5  dl_grid_model_130  0.6666666666666666  \n",
      "170                 0.1  0.5  0.0   dl_grid_model_48  0.6666666666666666  \n",
      "171                 0.2  0.5  0.0   dl_grid_model_76  0.6666666666666666  \n",
      "172                 0.2  0.5  0.0   dl_grid_model_85  0.6666666666666666  \n",
      "173                 0.2  0.5  0.5  dl_grid_model_190  0.6666666666666666  \n",
      "174                 0.1  0.5  0.5  dl_grid_model_155  0.6666666666666666  \n",
      "175                 0.1  0.5  0.5  dl_grid_model_162  0.6666666666666666  \n",
      "176                 0.1  0.5  0.5  dl_grid_model_158  0.6666666666666666  \n",
      "177                 0.1  0.5  0.0   dl_grid_model_64  0.6666666666666666  \n",
      "178                 0.1  0.5  0.0   dl_grid_model_70  0.6666666666666666  \n",
      "179                 0.2  0.5  0.0   dl_grid_model_79  0.6666666666666666  \n",
      "180                 0.2  0.5  0.0   dl_grid_model_78  0.6666666666666666  \n",
      "181                 0.2  0.5  0.0   dl_grid_model_92  0.6666666666666666  \n",
      "182                 0.1  0.0  0.5  dl_grid_model_104  0.6666666666666666  \n",
      "183                 0.2  0.5  0.5  dl_grid_model_174  0.6666666666666666  \n",
      "184                 0.1  0.5  0.0   dl_grid_model_50  0.6666666666666666  \n",
      "185                 0.1  0.5  0.0   dl_grid_model_59  0.6666666666666666  \n",
      "186                 0.1  0.0  0.5  dl_grid_model_119  0.6666666666666666  \n",
      "187                 0.2  0.5  0.0   dl_grid_model_74  0.6666666666666666  \n",
      "188                 0.1  0.5  0.0   dl_grid_model_69  0.6666666666666666  \n",
      "189                 0.1  0.5  0.0   dl_grid_model_56  0.6666666666666666  \n",
      "190                 0.1  0.5  0.5  dl_grid_model_150  0.6666666666666666  \n",
      "191                 0.2  0.5  0.5  dl_grid_model_188  0.6666666666666666  \n",
      "\n",
      "[192 rows x 9 columns]\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0493982681556\n",
      "RMSE: 0.222257211707\n",
      "LogLoss: 0.176911132793\n",
      "Mean Per-Class Error: 0.17261117697\n",
      "AUC: 0.905586456163\n",
      "Gini: 0.811172912325\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.767456739641: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11658.0</td>\n",
       "<td>239.0</td>\n",
       "<td>0.0201</td>\n",
       "<td> (239.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>304.0</td>\n",
       "<td>276.0</td>\n",
       "<td>0.5241</td>\n",
       "<td> (304.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11962.0</td>\n",
       "<td>515.0</td>\n",
       "<td>0.0435</td>\n",
       "<td> (543.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11658  239  0.0201   (239.0/11897.0)\n",
       "1      304    276  0.5241   (304.0/580.0)\n",
       "Total  11962  515  0.0435   (543.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.7674567</td>\n",
       "<td>0.5041096</td>\n",
       "<td>61.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3480278</td>\n",
       "<td>0.5578627</td>\n",
       "<td>173.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9039933</td>\n",
       "<td>0.5791284</td>\n",
       "<td>28.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9076071</td>\n",
       "<td>0.9625711</td>\n",
       "<td>27.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9917445</td>\n",
       "<td>0.9</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0075731</td>\n",
       "<td>1.0</td>\n",
       "<td>393.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9917445</td>\n",
       "<td>0.9999159</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.7674567</td>\n",
       "<td>0.4823650</td>\n",
       "<td>61.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1341745</td>\n",
       "<td>0.8227284</td>\n",
       "<td>263.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1243864</td>\n",
       "<td>0.8273888</td>\n",
       "<td>268.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.767457     0.50411   61\n",
       "max f2                       0.348028     0.557863  173\n",
       "max f0point5                 0.903993     0.579128  28\n",
       "max accuracy                 0.907607     0.962571  27\n",
       "max precision                0.991744     0.9       0\n",
       "max recall                   0.00757306   1         393\n",
       "max specificity              0.991744     0.999916  0\n",
       "max absolute_mcc             0.767457     0.482365  61\n",
       "max min_per_class_accuracy   0.134175     0.822728  263\n",
       "max mean_per_class_accuracy  0.124386     0.827389  268"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9513590</td>\n",
       "<td>16.6933655</td>\n",
       "<td>16.6933655</td>\n",
       "<td>0.776</td>\n",
       "<td>0.776</td>\n",
       "<td>0.1672414</td>\n",
       "<td>0.1672414</td>\n",
       "<td>1569.3365517</td>\n",
       "<td>1569.3365517</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9162353</td>\n",
       "<td>13.5956276</td>\n",
       "<td>15.1444966</td>\n",
       "<td>0.632</td>\n",
       "<td>0.704</td>\n",
       "<td>0.1362069</td>\n",
       "<td>0.3034483</td>\n",
       "<td>1259.5627586</td>\n",
       "<td>1414.4496552</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.8516258</td>\n",
       "<td>9.4653103</td>\n",
       "<td>13.2514345</td>\n",
       "<td>0.44</td>\n",
       "<td>0.616</td>\n",
       "<td>0.0948276</td>\n",
       "<td>0.3982759</td>\n",
       "<td>846.5310345</td>\n",
       "<td>1225.1434483</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.7773101</td>\n",
       "<td>6.7117655</td>\n",
       "<td>11.6165172</td>\n",
       "<td>0.312</td>\n",
       "<td>0.54</td>\n",
       "<td>0.0672414</td>\n",
       "<td>0.4655172</td>\n",
       "<td>571.1765517</td>\n",
       "<td>1061.6517241</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.6867062</td>\n",
       "<td>3.9901418</td>\n",
       "<td>10.1010196</td>\n",
       "<td>0.1854839</td>\n",
       "<td>0.4695513</td>\n",
       "<td>0.0396552</td>\n",
       "<td>0.5051724</td>\n",
       "<td>299.0141824</td>\n",
       "<td>910.1019562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.3593097</td>\n",
       "<td>3.5508704</td>\n",
       "<td>6.8259450</td>\n",
       "<td>0.1650641</td>\n",
       "<td>0.3173077</td>\n",
       "<td>0.1775862</td>\n",
       "<td>0.6827586</td>\n",
       "<td>255.0870358</td>\n",
       "<td>582.5944960</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.2154114</td>\n",
       "<td>1.7926724</td>\n",
       "<td>5.1481874</td>\n",
       "<td>0.0833333</td>\n",
       "<td>0.2393162</td>\n",
       "<td>0.0896552</td>\n",
       "<td>0.7724138</td>\n",
       "<td>79.2672414</td>\n",
       "<td>414.8187445</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.1410548</td>\n",
       "<td>0.8963362</td>\n",
       "<td>4.0852246</td>\n",
       "<td>0.0416667</td>\n",
       "<td>0.1899038</td>\n",
       "<td>0.0448276</td>\n",
       "<td>0.8172414</td>\n",
       "<td>-10.3663793</td>\n",
       "<td>308.5224635</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0731870</td>\n",
       "<td>0.7590465</td>\n",
       "<td>2.9770910</td>\n",
       "<td>0.0352847</td>\n",
       "<td>0.1383917</td>\n",
       "<td>0.0758621</td>\n",
       "<td>0.8931034</td>\n",
       "<td>-24.0953461</td>\n",
       "<td>197.7091030</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0453684</td>\n",
       "<td>0.3964564</td>\n",
       "<td>2.3318031</td>\n",
       "<td>0.0184295</td>\n",
       "<td>0.1083951</td>\n",
       "<td>0.0396552</td>\n",
       "<td>0.9327586</td>\n",
       "<td>-60.3543601</td>\n",
       "<td>133.1803108</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0323984</td>\n",
       "<td>0.2413213</td>\n",
       "<td>1.9136397</td>\n",
       "<td>0.0112179</td>\n",
       "<td>0.0889566</td>\n",
       "<td>0.0241379</td>\n",
       "<td>0.9568966</td>\n",
       "<td>-75.8678714</td>\n",
       "<td>91.3639730</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0255661</td>\n",
       "<td>0.1035063</td>\n",
       "<td>1.6121120</td>\n",
       "<td>0.0048115</td>\n",
       "<td>0.0749399</td>\n",
       "<td>0.0103448</td>\n",
       "<td>0.9672414</td>\n",
       "<td>-89.6493654</td>\n",
       "<td>61.2112034</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0206608</td>\n",
       "<td>0.1896096</td>\n",
       "<td>1.4088509</td>\n",
       "<td>0.0088141</td>\n",
       "<td>0.0654912</td>\n",
       "<td>0.0189655</td>\n",
       "<td>0.9862069</td>\n",
       "<td>-81.0390418</td>\n",
       "<td>40.8850864</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0164071</td>\n",
       "<td>0.0345021</td>\n",
       "<td>1.2371433</td>\n",
       "<td>0.0016038</td>\n",
       "<td>0.0575093</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9896552</td>\n",
       "<td>-96.5497885</td>\n",
       "<td>23.7143331</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0111647</td>\n",
       "<td>0.0689489</td>\n",
       "<td>1.1073093</td>\n",
       "<td>0.0032051</td>\n",
       "<td>0.0514739</td>\n",
       "<td>0.0068966</td>\n",
       "<td>0.9965517</td>\n",
       "<td>-93.1051061</td>\n",
       "<td>10.7309276</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0031002</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0034483</td>\n",
       "<td>1.0</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.951359           16.6934    16.6934            0.776            0.776                       0.167241        0.167241                   1569.34   1569.34\n",
       "    2        0.0200369                   0.916235           13.5956    15.1445            0.632            0.704                       0.136207        0.303448                   1259.56   1414.45\n",
       "    3        0.0300553                   0.851626           9.46531    13.2514            0.44             0.616                       0.0948276       0.398276                   846.531   1225.14\n",
       "    4        0.0400737                   0.77731            6.71177    11.6165            0.312            0.54                        0.0672414       0.465517                   571.177   1061.65\n",
       "    5        0.050012                    0.686706           3.99014    10.101             0.185484         0.469551                    0.0396552       0.505172                   299.014   910.102\n",
       "    6        0.100024                    0.35931            3.55087    6.82594            0.165064         0.317308                    0.177586        0.682759                   255.087   582.594\n",
       "    7        0.150036                    0.215411           1.79267    5.14819            0.0833333        0.239316                    0.0896552       0.772414                   79.2672   414.819\n",
       "    8        0.200048                    0.141055           0.896336   4.08522            0.0416667        0.189904                    0.0448276       0.817241                   -10.3664  308.522\n",
       "    9        0.299992                    0.073187           0.759047   2.97709            0.0352847        0.138392                    0.0758621       0.893103                   -24.0953  197.709\n",
       "    10       0.400016                    0.0453684          0.396456   2.3318             0.0184295        0.108395                    0.0396552       0.932759                   -60.3544  133.18\n",
       "    11       0.50004                     0.0323984          0.241321   1.91364            0.0112179        0.0889566                   0.0241379       0.956897                   -75.8679  91.364\n",
       "    12       0.599984                    0.0255661          0.103506   1.61211            0.00481155       0.0749399                   0.0103448       0.967241                   -89.6494  61.2112\n",
       "    13       0.700008                    0.0206608          0.18961    1.40885            0.0088141        0.0654912                   0.0189655       0.986207                   -81.039   40.8851\n",
       "    14       0.799952                    0.0164071          0.0345021  1.23714            0.00160385       0.0575093                   0.00344828      0.989655                   -96.5498  23.7143\n",
       "    15       0.899976                    0.0111647          0.0689489  1.10731            0.00320513       0.0514739                   0.00689655      0.996552                   -93.1051  10.7309\n",
       "    16       1                           0.00310017         0.0344745  1                  0.00160256       0.0464855                   0.00344828      1                          -96.5526  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning (DL)\n",
    "\n",
    "hyper_parameters = {'activation': ['tanh', 'tanh_with_dropout', 'rectifier', \n",
    "                                   'rectifier_with_dropout', 'maxout', 'maxout_with_dropout'],\n",
    "                    'epochs' : [5, 10],\n",
    "                    'hidden' : [[10, 10], [10, 10, 10]],\n",
    "                    'input_dropout_ratio' : [.1, .2],\n",
    "                    'l1' : [.0, .5],\n",
    "                    'l2' : [.0, .5]\n",
    "                   }\n",
    "\n",
    "dl_grid = H2OGridSearch(H2ODeepLearningEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                        grid_id='dl_grid',\n",
    "                        hyper_params=hyper_parameters)\n",
    "\n",
    "dl_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "dl_gridperf1 = dl_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(dl_gridperf1)\n",
    "\n",
    "# Grab the top DL model, chosen by validation F1\n",
    "best_dl = dl_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_dl_perf1 = best_dl.model_performance(test_h2o)\n",
    "print(best_dl_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "     activation              epochs    hidden input_dropout_ratio    l1    l2  \\\n",
      "0        Maxout  10.400028398673907  [20, 20]                 0.1   0.0   0.0   \n",
      "1          Tanh  10.400028398673907  [20, 20]                 0.1   0.0   0.0   \n",
      "2          Tanh  10.400028398673907  [10, 10]                 0.1   0.0   0.0   \n",
      "3          Tanh  10.400028398673907  [20, 20]                 0.1   0.0  0.01   \n",
      "4        Maxout   5.197171810615108  [10, 10]                 0.1  0.01   0.0   \n",
      "5        Maxout   5.197171810615108  [20, 20]                 0.1   0.0   0.0   \n",
      "6        Maxout  10.400028398673907  [10, 10]                 0.1  0.01   0.0   \n",
      "7          Tanh   5.197171810615108  [10, 10]                 0.1  0.01   0.0   \n",
      "8        Maxout  10.400028398673907  [20, 20]                 0.1  0.01  0.01   \n",
      "9        Maxout  10.400028398673907  [10, 10]                 0.1   0.0   0.0   \n",
      "10       Maxout  10.400028398673907  [20, 20]                 0.1  0.01   0.0   \n",
      "11       Maxout   5.197171810615108  [20, 20]                 0.1  0.01   0.0   \n",
      "12       Maxout  10.400028398673907  [20, 20]                 0.1   0.0  0.01   \n",
      "13       Maxout   5.197171810615108  [10, 10]                 0.1  0.01  0.01   \n",
      "14       Maxout   5.197171810615108  [20, 20]                 0.1  0.01  0.01   \n",
      "15         Tanh  10.400028398673907  [10, 10]                 0.1   0.0  0.01   \n",
      "16       Maxout   5.197171810615108  [20, 20]                 0.1   0.0  0.01   \n",
      "17       Maxout   5.197171810615108  [10, 10]                 0.1   0.0   0.0   \n",
      "18       Maxout  10.400028398673907  [10, 10]                 0.1  0.01  0.01   \n",
      "19         Tanh  10.400028398673907  [10, 10]                 0.1  0.01   0.0   \n",
      "20         Tanh   5.197171810615108  [20, 20]                 0.1   0.0   0.0   \n",
      "21         Tanh   5.197171810615108  [10, 10]                 0.1   0.0   0.0   \n",
      "22       Maxout   5.197171810615108  [10, 10]                 0.1   0.0  0.01   \n",
      "23       Maxout  10.400028398673907  [10, 10]                 0.1   0.0  0.01   \n",
      "24         Tanh   5.197171810615108  [10, 10]                 0.1  0.01  0.01   \n",
      "25         Tanh  10.400028398673907  [20, 20]                 0.1  0.01   0.0   \n",
      "26         Tanh   5.197171810615108  [10, 10]                 0.1   0.0  0.01   \n",
      "27         Tanh   5.197171810615108  [20, 20]                 0.1  0.01   0.0   \n",
      "28         Tanh   5.197171810615108  [20, 20]                 0.1  0.01  0.01   \n",
      "29         Tanh  10.400028398673907  [20, 20]                 0.1  0.01  0.01   \n",
      "30         Tanh   5.197171810615108  [20, 20]                 0.1   0.0  0.01   \n",
      "31         Tanh  10.400028398673907  [10, 10]                 0.1  0.01  0.01   \n",
      "\n",
      "             model_ids                  f1  \n",
      "0    dl_grid_1_model_7  0.8585392019548828  \n",
      "1    dl_grid_1_model_6  0.8485426733556986  \n",
      "2    dl_grid_1_model_2   0.847408324307597  \n",
      "3   dl_grid_1_model_22  0.8271109997494362  \n",
      "4    dl_grid_1_model_9  0.8179259579995669  \n",
      "5    dl_grid_1_model_5  0.8134675628820827  \n",
      "6   dl_grid_1_model_11  0.8101017530955007  \n",
      "7    dl_grid_1_model_8  0.8095820840726917  \n",
      "8   dl_grid_1_model_31   0.807056650638474  \n",
      "9    dl_grid_1_model_3  0.8067730198712567  \n",
      "10  dl_grid_1_model_15  0.8021954566244855  \n",
      "11  dl_grid_1_model_13  0.8020377584656878  \n",
      "12  dl_grid_1_model_23  0.7989825119236884  \n",
      "13  dl_grid_1_model_25  0.7986766917293233  \n",
      "14  dl_grid_1_model_29  0.7966172603394693  \n",
      "15  dl_grid_1_model_18  0.7934549870569811  \n",
      "16  dl_grid_1_model_21  0.7913358646876079  \n",
      "17   dl_grid_1_model_1  0.7891383412179716  \n",
      "18  dl_grid_1_model_27  0.7879902687948732  \n",
      "19  dl_grid_1_model_10  0.7766638819270397  \n",
      "20   dl_grid_1_model_4  0.7703470727804448  \n",
      "21   dl_grid_1_model_0    0.76929358922009  \n",
      "22  dl_grid_1_model_17  0.7681120387826555  \n",
      "23  dl_grid_1_model_19  0.7633502860775587  \n",
      "24  dl_grid_1_model_24  0.7628291370363252  \n",
      "25  dl_grid_1_model_14  0.7578690432265207  \n",
      "26  dl_grid_1_model_16   0.755153227433909  \n",
      "27  dl_grid_1_model_12  0.7420592624173951  \n",
      "28  dl_grid_1_model_28  0.7382935789310291  \n",
      "29  dl_grid_1_model_30  0.7356673088420912  \n",
      "30  dl_grid_1_model_20  0.7282095319879777  \n",
      "31  dl_grid_1_model_26  0.6906971531046852  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.264634883751\n",
      "RMSE: 0.514426752562\n",
      "LogLoss: 0.960210710166\n",
      "Mean Per-Class Error: 0.169226449438\n",
      "AUC: 0.909536379789\n",
      "Gini: 0.819072759577\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.995860358147: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11591.0</td>\n",
       "<td>306.0</td>\n",
       "<td>0.0257</td>\n",
       "<td> (306.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>289.0</td>\n",
       "<td>291.0</td>\n",
       "<td>0.4983</td>\n",
       "<td> (289.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11880.0</td>\n",
       "<td>597.0</td>\n",
       "<td>0.0477</td>\n",
       "<td> (595.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11591  306  0.0257   (306.0/11897.0)\n",
       "1      289    291  0.4983   (289.0/580.0)\n",
       "Total  11880  597  0.0477   (595.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.9958604</td>\n",
       "<td>0.4944775</td>\n",
       "<td>3.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.9789641</td>\n",
       "<td>0.5583531</td>\n",
       "<td>13.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9998737</td>\n",
       "<td>0.5421053</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9998737</td>\n",
       "<td>0.9600866</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998737</td>\n",
       "<td>0.6242424</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0037693</td>\n",
       "<td>1.0</td>\n",
       "<td>394.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998737</td>\n",
       "<td>0.9895772</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9974264</td>\n",
       "<td>0.4696756</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.8524545</td>\n",
       "<td>0.8275862</td>\n",
       "<td>66.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.8279650</td>\n",
       "<td>0.8307736</td>\n",
       "<td>75.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.99586      0.494477  3\n",
       "max f2                       0.978964     0.558353  13\n",
       "max f0point5                 0.999874     0.542105  0\n",
       "max accuracy                 0.999874     0.960087  0\n",
       "max precision                0.999874     0.624242  0\n",
       "max recall                   0.00376929   1         394\n",
       "max specificity              0.999874     0.989577  0\n",
       "max absolute_mcc             0.997426     0.469676  2\n",
       "max min_per_class_accuracy   0.852454     0.827586  66\n",
       "max mean_per_class_accuracy  0.827965     0.830774  75"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9999968</td>\n",
       "<td>18.2422345</td>\n",
       "<td>18.2422345</td>\n",
       "<td>0.848</td>\n",
       "<td>0.848</td>\n",
       "<td>0.1827586</td>\n",
       "<td>0.1827586</td>\n",
       "<td>1724.2234483</td>\n",
       "<td>1724.2234483</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9998050</td>\n",
       "<td>10.8420828</td>\n",
       "<td>14.5421586</td>\n",
       "<td>0.504</td>\n",
       "<td>0.676</td>\n",
       "<td>0.1086207</td>\n",
       "<td>0.2913793</td>\n",
       "<td>984.2082759</td>\n",
       "<td>1354.2158621</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.9988860</td>\n",
       "<td>9.1211172</td>\n",
       "<td>12.7351448</td>\n",
       "<td>0.424</td>\n",
       "<td>0.592</td>\n",
       "<td>0.0913793</td>\n",
       "<td>0.3827586</td>\n",
       "<td>812.1117241</td>\n",
       "<td>1173.5144828</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.9971544</td>\n",
       "<td>7.5722483</td>\n",
       "<td>11.4444207</td>\n",
       "<td>0.352</td>\n",
       "<td>0.532</td>\n",
       "<td>0.0758621</td>\n",
       "<td>0.4586207</td>\n",
       "<td>657.2248276</td>\n",
       "<td>1044.4420690</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.9946912</td>\n",
       "<td>4.6840795</td>\n",
       "<td>10.1010196</td>\n",
       "<td>0.2177419</td>\n",
       "<td>0.4695513</td>\n",
       "<td>0.0465517</td>\n",
       "<td>0.5051724</td>\n",
       "<td>368.4079533</td>\n",
       "<td>910.1019562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.9675925</td>\n",
       "<td>3.5853448</td>\n",
       "<td>6.8431822</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.3181090</td>\n",
       "<td>0.1793103</td>\n",
       "<td>0.6844828</td>\n",
       "<td>258.5344828</td>\n",
       "<td>584.3182195</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.9179515</td>\n",
       "<td>1.7237235</td>\n",
       "<td>5.1366960</td>\n",
       "<td>0.0801282</td>\n",
       "<td>0.2387821</td>\n",
       "<td>0.0862069</td>\n",
       "<td>0.7706897</td>\n",
       "<td>72.3723475</td>\n",
       "<td>413.6695955</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.8534602</td>\n",
       "<td>1.0342341</td>\n",
       "<td>4.1110805</td>\n",
       "<td>0.0480769</td>\n",
       "<td>0.1911058</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.8224138</td>\n",
       "<td>3.4234085</td>\n",
       "<td>311.1080487</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.6789228</td>\n",
       "<td>0.6900423</td>\n",
       "<td>2.9713437</td>\n",
       "<td>0.0320770</td>\n",
       "<td>0.1381245</td>\n",
       "<td>0.0689655</td>\n",
       "<td>0.8913793</td>\n",
       "<td>-30.9957692</td>\n",
       "<td>197.1343750</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.5009849</td>\n",
       "<td>0.4481681</td>\n",
       "<td>2.3404235</td>\n",
       "<td>0.0208333</td>\n",
       "<td>0.1087958</td>\n",
       "<td>0.0448276</td>\n",
       "<td>0.9362069</td>\n",
       "<td>-55.1831897</td>\n",
       "<td>134.0423452</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.3278894</td>\n",
       "<td>0.2930330</td>\n",
       "<td>1.9308797</td>\n",
       "<td>0.0136218</td>\n",
       "<td>0.0897580</td>\n",
       "<td>0.0293103</td>\n",
       "<td>0.9655172</td>\n",
       "<td>-70.6967009</td>\n",
       "<td>93.0879728</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.1899042</td>\n",
       "<td>0.2070127</td>\n",
       "<td>1.6437221</td>\n",
       "<td>0.0096231</td>\n",
       "<td>0.0764093</td>\n",
       "<td>0.0206897</td>\n",
       "<td>0.9862069</td>\n",
       "<td>-79.2987307</td>\n",
       "<td>64.3722074</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0916266</td>\n",
       "<td>0.0517117</td>\n",
       "<td>1.4162399</td>\n",
       "<td>0.0024038</td>\n",
       "<td>0.0658347</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9913793</td>\n",
       "<td>-94.8288296</td>\n",
       "<td>41.6239942</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0309825</td>\n",
       "<td>0.0345021</td>\n",
       "<td>1.2436092</td>\n",
       "<td>0.0016038</td>\n",
       "<td>0.0578098</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-96.5497885</td>\n",
       "<td>24.3609237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0043158</td>\n",
       "<td>0.0172372</td>\n",
       "<td>1.1073093</td>\n",
       "<td>0.0008013</td>\n",
       "<td>0.0514739</td>\n",
       "<td>0.0017241</td>\n",
       "<td>0.9965517</td>\n",
       "<td>-98.2762765</td>\n",
       "<td>10.7309276</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0034483</td>\n",
       "<td>1.0</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.999997           18.2422    18.2422            0.848            0.848                       0.182759        0.182759                   1724.22   1724.22\n",
       "    2        0.0200369                   0.999805           10.8421    14.5422            0.504            0.676                       0.108621        0.291379                   984.208   1354.22\n",
       "    3        0.0300553                   0.998886           9.12112    12.7351            0.424            0.592                       0.0913793       0.382759                   812.112   1173.51\n",
       "    4        0.0400737                   0.997154           7.57225    11.4444            0.352            0.532                       0.0758621       0.458621                   657.225   1044.44\n",
       "    5        0.050012                    0.994691           4.68408    10.101             0.217742         0.469551                    0.0465517       0.505172                   368.408   910.102\n",
       "    6        0.100024                    0.967592           3.58534    6.84318            0.166667         0.318109                    0.17931         0.684483                   258.534   584.318\n",
       "    7        0.150036                    0.917952           1.72372    5.1367             0.0801282        0.238782                    0.0862069       0.77069                    72.3723   413.67\n",
       "    8        0.200048                    0.85346            1.03423    4.11108            0.0480769        0.191106                    0.0517241       0.822414                   3.42341   311.108\n",
       "    9        0.299992                    0.678923           0.690042   2.97134            0.032077         0.138124                    0.0689655       0.891379                   -30.9958  197.134\n",
       "    10       0.400016                    0.500985           0.448168   2.34042            0.0208333        0.108796                    0.0448276       0.936207                   -55.1832  134.042\n",
       "    11       0.50004                     0.327889           0.293033   1.93088            0.0136218        0.089758                    0.0293103       0.965517                   -70.6967  93.088\n",
       "    12       0.599984                    0.189904           0.207013   1.64372            0.0096231        0.0764093                   0.0206897       0.986207                   -79.2987  64.3722\n",
       "    13       0.700008                    0.0916266          0.0517117  1.41624            0.00240385       0.0658347                   0.00517241      0.991379                   -94.8288  41.624\n",
       "    14       0.799952                    0.0309825          0.0345021  1.24361            0.00160385       0.0578098                   0.00344828      0.994828                   -96.5498  24.3609\n",
       "    15       0.899976                    0.00431577         0.0172372  1.10731            0.000801282      0.0514739                   0.00172414      0.996552                   -98.2763  10.7309\n",
       "    16       1                           4.46352e-11        0.0344745  1                  0.00160256       0.0464855                   0.00344828      1                          -96.5526  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning (DL) - Refinement 1\n",
    "\n",
    "hyper_parameters = {'activation': ['tanh', 'maxout'],\n",
    "                    'epochs' : [5, 10],\n",
    "                    'hidden' : [[10, 10], [20, 20]],\n",
    "                    'input_dropout_ratio' : [.1],\n",
    "                    'l1' : [.0, 1e-2],\n",
    "                    'l2' : [.0, 1e-2]\n",
    "                   }\n",
    "\n",
    "dl_grid = H2OGridSearch(H2ODeepLearningEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                        grid_id='dl_grid_1',\n",
    "                        hyper_params=hyper_parameters)\n",
    "\n",
    "dl_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "dl_gridperf1 = dl_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(dl_gridperf1)\n",
    "\n",
    "# Grab the top DL model, chosen by validation F1\n",
    "best_dl = dl_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_dl_perf1 = best_dl.model_performance(test_h2o)\n",
    "print(best_dl_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "    activation              epochs    hidden input_dropout_ratio   l1   l2  \\\n",
      "0       Maxout  10.400028398673907  [30, 30]                 0.1  0.0  0.0   \n",
      "1         Tanh  10.400028398673907  [30, 30]                 0.1  0.0  0.0   \n",
      "2       Maxout  10.400028398673907  [20, 20]                 0.1  0.0  0.0   \n",
      "3         Tanh  10.400028398673907  [20, 20]                 0.1  0.0  0.0   \n",
      "\n",
      "           model_ids                  f1  \n",
      "0  dl_grid_2_model_3  0.8606990725355729  \n",
      "1  dl_grid_2_model_2  0.8604917981952928  \n",
      "2  dl_grid_2_model_1  0.8238240028617422  \n",
      "3  dl_grid_2_model_0  0.8168299783990233  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.037159680504\n",
      "RMSE: 0.192768463458\n",
      "LogLoss: 0.159378928779\n",
      "Mean Per-Class Error: 0.188456304545\n",
      "AUC: 0.886346818816\n",
      "Gini: 0.772693637631\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.532853189568: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11696.0</td>\n",
       "<td>201.0</td>\n",
       "<td>0.0169</td>\n",
       "<td> (201.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>348.0</td>\n",
       "<td>232.0</td>\n",
       "<td>0.6</td>\n",
       "<td> (348.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12044.0</td>\n",
       "<td>433.0</td>\n",
       "<td>0.044</td>\n",
       "<td> (549.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11696  201  0.0169   (201.0/11897.0)\n",
       "1      348    232  0.6      (348.0/580.0)\n",
       "Total  12044  433  0.044    (549.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5328532</td>\n",
       "<td>0.4580454</td>\n",
       "<td>119.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0924643</td>\n",
       "<td>0.5240913</td>\n",
       "<td>278.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9041665</td>\n",
       "<td>0.5336676</td>\n",
       "<td>32.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9041665</td>\n",
       "<td>0.9610483</td>\n",
       "<td>32.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999477</td>\n",
       "<td>0.8846154</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000169</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999477</td>\n",
       "<td>0.9994957</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5328532</td>\n",
       "<td>0.4406778</td>\n",
       "<td>119.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0171236</td>\n",
       "<td>0.8034483</td>\n",
       "<td>350.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0295977</td>\n",
       "<td>0.8115437</td>\n",
       "<td>331.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.532853     0.458045  119\n",
       "max f2                       0.0924643    0.524091  278\n",
       "max f0point5                 0.904167     0.533668  32\n",
       "max accuracy                 0.904167     0.961048  32\n",
       "max precision                0.999948     0.884615  0\n",
       "max recall                   1.69313e-05  1         399\n",
       "max specificity              0.999948     0.999496  0\n",
       "max absolute_mcc             0.532853     0.440678  119\n",
       "max min_per_class_accuracy   0.0171236    0.803448  350\n",
       "max mean_per_class_accuracy  0.0295977    0.811544  331"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9834693</td>\n",
       "<td>17.3817517</td>\n",
       "<td>17.3817517</td>\n",
       "<td>0.808</td>\n",
       "<td>0.808</td>\n",
       "<td>0.1741379</td>\n",
       "<td>0.1741379</td>\n",
       "<td>1638.1751724</td>\n",
       "<td>1638.1751724</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.8379521</td>\n",
       "<td>10.6699862</td>\n",
       "<td>14.0258690</td>\n",
       "<td>0.496</td>\n",
       "<td>0.652</td>\n",
       "<td>0.1068966</td>\n",
       "<td>0.2810345</td>\n",
       "<td>966.9986207</td>\n",
       "<td>1302.5868965</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.6239138</td>\n",
       "<td>7.9164414</td>\n",
       "<td>11.9893931</td>\n",
       "<td>0.368</td>\n",
       "<td>0.5573333</td>\n",
       "<td>0.0793103</td>\n",
       "<td>0.3603448</td>\n",
       "<td>691.6441379</td>\n",
       "<td>1098.9393103</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.4466363</td>\n",
       "<td>6.0233793</td>\n",
       "<td>10.4978897</td>\n",
       "<td>0.28</td>\n",
       "<td>0.488</td>\n",
       "<td>0.0603448</td>\n",
       "<td>0.4206897</td>\n",
       "<td>502.3379310</td>\n",
       "<td>949.7889655</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.3204781</td>\n",
       "<td>5.2045328</td>\n",
       "<td>9.4460046</td>\n",
       "<td>0.2419355</td>\n",
       "<td>0.4391026</td>\n",
       "<td>0.0517241</td>\n",
       "<td>0.4724138</td>\n",
       "<td>420.4532814</td>\n",
       "<td>844.6004642</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0883737</td>\n",
       "<td>3.4129725</td>\n",
       "<td>6.4294886</td>\n",
       "<td>0.1586538</td>\n",
       "<td>0.2988782</td>\n",
       "<td>0.1706897</td>\n",
       "<td>0.6431034</td>\n",
       "<td>241.2972480</td>\n",
       "<td>542.9488561</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0400655</td>\n",
       "<td>1.8960958</td>\n",
       "<td>4.9183576</td>\n",
       "<td>0.0881410</td>\n",
       "<td>0.2286325</td>\n",
       "<td>0.0948276</td>\n",
       "<td>0.7379310</td>\n",
       "<td>89.6095822</td>\n",
       "<td>391.8357648</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0210125</td>\n",
       "<td>0.9652851</td>\n",
       "<td>3.9300895</td>\n",
       "<td>0.0448718</td>\n",
       "<td>0.1826923</td>\n",
       "<td>0.0482759</td>\n",
       "<td>0.7862069</td>\n",
       "<td>-3.4714854</td>\n",
       "<td>293.0089523</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0075118</td>\n",
       "<td>0.7072934</td>\n",
       "<td>2.8563982</td>\n",
       "<td>0.0328789</td>\n",
       "<td>0.1327812</td>\n",
       "<td>0.0706897</td>\n",
       "<td>0.8568966</td>\n",
       "<td>-29.2706634</td>\n",
       "<td>185.6398150</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0032609</td>\n",
       "<td>0.4309309</td>\n",
       "<td>2.2499098</td>\n",
       "<td>0.0200321</td>\n",
       "<td>0.1045883</td>\n",
       "<td>0.0431034</td>\n",
       "<td>0.9</td>\n",
       "<td>-56.9069131</td>\n",
       "<td>124.9909838</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0014269</td>\n",
       "<td>0.3275075</td>\n",
       "<td>1.8653677</td>\n",
       "<td>0.0152244</td>\n",
       "<td>0.0867126</td>\n",
       "<td>0.0327586</td>\n",
       "<td>0.9327586</td>\n",
       "<td>-67.2492540</td>\n",
       "<td>86.5367737</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0005918</td>\n",
       "<td>0.3105190</td>\n",
       "<td>1.6063648</td>\n",
       "<td>0.0144346</td>\n",
       "<td>0.0746727</td>\n",
       "<td>0.0310345</td>\n",
       "<td>0.9637931</td>\n",
       "<td>-68.9480961</td>\n",
       "<td>60.6364754</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0002159</td>\n",
       "<td>0.1896096</td>\n",
       "<td>1.4039248</td>\n",
       "<td>0.0088141</td>\n",
       "<td>0.0652622</td>\n",
       "<td>0.0189655</td>\n",
       "<td>0.9827586</td>\n",
       "<td>-81.0390418</td>\n",
       "<td>40.3924812</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0000555</td>\n",
       "<td>0.1207574</td>\n",
       "<td>1.2436092</td>\n",
       "<td>0.0056135</td>\n",
       "<td>0.0578098</td>\n",
       "<td>0.0120690</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-87.9242596</td>\n",
       "<td>24.3609237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0000054</td>\n",
       "<td>0.0517117</td>\n",
       "<td>1.1111408</td>\n",
       "<td>0.0024038</td>\n",
       "<td>0.0516520</td>\n",
       "<td>0.0051724</td>\n",
       "<td>1.0</td>\n",
       "<td>-94.8288296</td>\n",
       "<td>11.1140796</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.983469           17.3818    17.3818            0.808            0.808                       0.174138        0.174138                   1638.18   1638.18\n",
       "    2        0.0200369                   0.837952           10.67      14.0259            0.496            0.652                       0.106897        0.281034                   966.999   1302.59\n",
       "    3        0.0300553                   0.623914           7.91644    11.9894            0.368            0.557333                    0.0793103       0.360345                   691.644   1098.94\n",
       "    4        0.0400737                   0.446636           6.02338    10.4979            0.28             0.488                       0.0603448       0.42069                    502.338   949.789\n",
       "    5        0.050012                    0.320478           5.20453    9.446              0.241935         0.439103                    0.0517241       0.472414                   420.453   844.6\n",
       "    6        0.100024                    0.0883737          3.41297    6.42949            0.158654         0.298878                    0.17069         0.643103                   241.297   542.949\n",
       "    7        0.150036                    0.0400655          1.8961     4.91836            0.088141         0.228632                    0.0948276       0.737931                   89.6096   391.836\n",
       "    8        0.200048                    0.0210125          0.965285   3.93009            0.0448718        0.182692                    0.0482759       0.786207                   -3.47149  293.009\n",
       "    9        0.299992                    0.00751185         0.707293   2.8564             0.0328789        0.132781                    0.0706897       0.856897                   -29.2707  185.64\n",
       "    10       0.400016                    0.00326091         0.430931   2.24991            0.0200321        0.104588                    0.0431034       0.9                        -56.9069  124.991\n",
       "    11       0.50004                     0.00142689         0.327507   1.86537            0.0152244        0.0867126                   0.0327586       0.932759                   -67.2493  86.5368\n",
       "    12       0.599984                    0.00059181         0.310519   1.60636            0.0144346        0.0746727                   0.0310345       0.963793                   -68.9481  60.6365\n",
       "    13       0.700008                    0.000215913        0.18961    1.40392            0.0088141        0.0652622                   0.0189655       0.982759                   -81.039   40.3925\n",
       "    14       0.799952                    5.55488e-05        0.120757   1.24361            0.00561347       0.0578098                   0.012069        0.994828                   -87.9243  24.3609\n",
       "    15       0.899976                    5.41739e-06        0.0517117  1.11114            0.00240385       0.051652                    0.00517241      1                          -94.8288  11.1141\n",
       "    16       1                           4.82793e-15        0          1                  0                0.0464855                   0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning (DL) - Refinement 2\n",
    "\n",
    "hyper_parameters = {'activation': ['tanh', 'maxout'],\n",
    "                    'epochs' : [10],\n",
    "                    'hidden' : [[20, 20], [30, 30]],\n",
    "                    'input_dropout_ratio' : [.1],\n",
    "                    'l1' : [.0],\n",
    "                    'l2' : [.0]\n",
    "                   }\n",
    "\n",
    "dl_grid = H2OGridSearch(H2ODeepLearningEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                        grid_id='dl_grid_2',\n",
    "                        hyper_params=hyper_parameters)\n",
    "\n",
    "dl_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "dl_gridperf1 = dl_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(dl_gridperf1)\n",
    "\n",
    "# Grab the top DL model, chosen by validation F1\n",
    "best_dl = dl_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_dl_perf1 = best_dl.model_performance(test_h2o)\n",
    "print(best_dl_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "    activation             epochs    hidden input_dropout_ratio   l1   l2  \\\n",
      "0         Tanh  5.197171810615108  [30, 30]                 0.1  0.0  0.0   \n",
      "1       Maxout  5.197171810615108  [30, 30]                 0.1  0.0  0.0   \n",
      "2       Maxout  5.197171810615108  [50, 50]                 0.1  0.0  0.0   \n",
      "3         Tanh  5.197171810615108  [50, 50]                 0.1  0.0  0.0   \n",
      "\n",
      "           model_ids                  f1  \n",
      "0  dl_grid_3_model_0  0.8567309425121644  \n",
      "1  dl_grid_3_model_1  0.8564667334122144  \n",
      "2  dl_grid_3_model_3  0.8538049555647107  \n",
      "3  dl_grid_3_model_2  0.8134379981352823  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.229073907109\n",
      "RMSE: 0.478616659875\n",
      "LogLoss: 0.711572438437\n",
      "Mean Per-Class Error: 0.185894879323\n",
      "AUC: 0.892766504451\n",
      "Gini: 0.785533008901\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.964041076497: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11560.0</td>\n",
       "<td>337.0</td>\n",
       "<td>0.0283</td>\n",
       "<td> (337.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>308.0</td>\n",
       "<td>272.0</td>\n",
       "<td>0.531</td>\n",
       "<td> (308.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11868.0</td>\n",
       "<td>609.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (645.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11560  337  0.0283   (337.0/11897.0)\n",
       "1      308    272  0.531    (308.0/580.0)\n",
       "Total  11868  609  0.0517   (645.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.9640411</td>\n",
       "<td>0.4575273</td>\n",
       "<td>16.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.9061748</td>\n",
       "<td>0.5192630</td>\n",
       "<td>38.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.9792652</td>\n",
       "<td>0.4746241</td>\n",
       "<td>9.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.9897625</td>\n",
       "<td>0.9572013</td>\n",
       "<td>4.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9949828</td>\n",
       "<td>0.7375</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0265075</td>\n",
       "<td>1.0</td>\n",
       "<td>384.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9970250</td>\n",
       "<td>0.9989913</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.9640411</td>\n",
       "<td>0.4305447</td>\n",
       "<td>16.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.7210509</td>\n",
       "<td>0.8091956</td>\n",
       "<td>103.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.7001502</td>\n",
       "<td>0.8141051</td>\n",
       "<td>111.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.964041     0.457527  16\n",
       "max f2                       0.906175     0.519263  38\n",
       "max f0point5                 0.979265     0.474624  9\n",
       "max accuracy                 0.989762     0.957201  4\n",
       "max precision                0.994983     0.7375    1\n",
       "max recall                   0.0265075    1         384\n",
       "max specificity              0.997025     0.998991  0\n",
       "max absolute_mcc             0.964041     0.430545  16\n",
       "max min_per_class_accuracy   0.721051     0.809196  103\n",
       "max mean_per_class_accuracy  0.70015      0.814105  111"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.9922666</td>\n",
       "<td>14.4561103</td>\n",
       "<td>14.4561103</td>\n",
       "<td>0.672</td>\n",
       "<td>0.672</td>\n",
       "<td>0.1448276</td>\n",
       "<td>0.1448276</td>\n",
       "<td>1345.6110345</td>\n",
       "<td>1345.6110345</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.9858732</td>\n",
       "<td>10.8420828</td>\n",
       "<td>12.6490966</td>\n",
       "<td>0.504</td>\n",
       "<td>0.588</td>\n",
       "<td>0.1086207</td>\n",
       "<td>0.2534483</td>\n",
       "<td>984.2082759</td>\n",
       "<td>1164.9096552</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.9789595</td>\n",
       "<td>8.4327310</td>\n",
       "<td>11.2436414</td>\n",
       "<td>0.392</td>\n",
       "<td>0.5226667</td>\n",
       "<td>0.0844828</td>\n",
       "<td>0.3379310</td>\n",
       "<td>743.2731034</td>\n",
       "<td>1024.3641379</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.9715516</td>\n",
       "<td>6.7117655</td>\n",
       "<td>10.1106724</td>\n",
       "<td>0.312</td>\n",
       "<td>0.47</td>\n",
       "<td>0.0672414</td>\n",
       "<td>0.4051724</td>\n",
       "<td>571.1765517</td>\n",
       "<td>911.0672414</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.9615102</td>\n",
       "<td>6.7658927</td>\n",
       "<td>9.4460046</td>\n",
       "<td>0.3145161</td>\n",
       "<td>0.4391026</td>\n",
       "<td>0.0672414</td>\n",
       "<td>0.4724138</td>\n",
       "<td>576.5892659</td>\n",
       "<td>844.6004642</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.9062885</td>\n",
       "<td>3.2406001</td>\n",
       "<td>6.3433024</td>\n",
       "<td>0.1506410</td>\n",
       "<td>0.2948718</td>\n",
       "<td>0.1620690</td>\n",
       "<td>0.6344828</td>\n",
       "<td>224.0600133</td>\n",
       "<td>534.3302387</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.8387757</td>\n",
       "<td>1.7926724</td>\n",
       "<td>4.8264257</td>\n",
       "<td>0.0833333</td>\n",
       "<td>0.2243590</td>\n",
       "<td>0.0896552</td>\n",
       "<td>0.7241379</td>\n",
       "<td>79.2672414</td>\n",
       "<td>382.6425729</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.7516000</td>\n",
       "<td>1.2066064</td>\n",
       "<td>3.9214709</td>\n",
       "<td>0.0560897</td>\n",
       "<td>0.1822917</td>\n",
       "<td>0.0603448</td>\n",
       "<td>0.7844828</td>\n",
       "<td>20.6606432</td>\n",
       "<td>292.1470905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.5879874</td>\n",
       "<td>0.8970550</td>\n",
       "<td>2.9138709</td>\n",
       "<td>0.0417001</td>\n",
       "<td>0.1354528</td>\n",
       "<td>0.0896552</td>\n",
       "<td>0.8741379</td>\n",
       "<td>-10.2944999</td>\n",
       "<td>191.3870950</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.4437532</td>\n",
       "<td>0.4826426</td>\n",
       "<td>2.3059421</td>\n",
       "<td>0.0224359</td>\n",
       "<td>0.1071929</td>\n",
       "<td>0.0482759</td>\n",
       "<td>0.9224138</td>\n",
       "<td>-51.7357427</td>\n",
       "<td>130.5942075</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.3291577</td>\n",
       "<td>0.2757958</td>\n",
       "<td>1.8998477</td>\n",
       "<td>0.0128205</td>\n",
       "<td>0.0883154</td>\n",
       "<td>0.0275862</td>\n",
       "<td>0.95</td>\n",
       "<td>-72.4204244</td>\n",
       "<td>89.9847732</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.2322334</td>\n",
       "<td>0.2070127</td>\n",
       "<td>1.6178593</td>\n",
       "<td>0.0096231</td>\n",
       "<td>0.0752071</td>\n",
       "<td>0.0206897</td>\n",
       "<td>0.9706897</td>\n",
       "<td>-79.2987307</td>\n",
       "<td>61.7859314</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.1519785</td>\n",
       "<td>0.1723723</td>\n",
       "<td>1.4113139</td>\n",
       "<td>0.0080128</td>\n",
       "<td>0.0656057</td>\n",
       "<td>0.0172414</td>\n",
       "<td>0.9879310</td>\n",
       "<td>-82.7627653</td>\n",
       "<td>41.1313890</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0817677</td>\n",
       "<td>0.0862553</td>\n",
       "<td>1.2457645</td>\n",
       "<td>0.0040096</td>\n",
       "<td>0.0579100</td>\n",
       "<td>0.0086207</td>\n",
       "<td>0.9965517</td>\n",
       "<td>-91.3744711</td>\n",
       "<td>24.5764539</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0324959</td>\n",
       "<td>0.0172372</td>\n",
       "<td>1.1092250</td>\n",
       "<td>0.0008013</td>\n",
       "<td>0.0515629</td>\n",
       "<td>0.0017241</td>\n",
       "<td>0.9982759</td>\n",
       "<td>-98.2762765</td>\n",
       "<td>10.9225036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0011283</td>\n",
       "<td>0.0172372</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0008013</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0017241</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.2762765</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.992267           14.4561    14.4561            0.672            0.672                       0.144828        0.144828                   1345.61   1345.61\n",
       "    2        0.0200369                   0.985873           10.8421    12.6491            0.504            0.588                       0.108621        0.253448                   984.208   1164.91\n",
       "    3        0.0300553                   0.978959           8.43273    11.2436            0.392            0.522667                    0.0844828       0.337931                   743.273   1024.36\n",
       "    4        0.0400737                   0.971552           6.71177    10.1107            0.312            0.47                        0.0672414       0.405172                   571.177   911.067\n",
       "    5        0.050012                    0.96151            6.76589    9.446              0.314516         0.439103                    0.0672414       0.472414                   576.589   844.6\n",
       "    6        0.100024                    0.906288           3.2406     6.3433             0.150641         0.294872                    0.162069        0.634483                   224.06    534.33\n",
       "    7        0.150036                    0.838776           1.79267    4.82643            0.0833333        0.224359                    0.0896552       0.724138                   79.2672   382.643\n",
       "    8        0.200048                    0.7516             1.20661    3.92147            0.0560897        0.182292                    0.0603448       0.784483                   20.6606   292.147\n",
       "    9        0.299992                    0.587987           0.897055   2.91387            0.0417001        0.135453                    0.0896552       0.874138                   -10.2945  191.387\n",
       "    10       0.400016                    0.443753           0.482643   2.30594            0.0224359        0.107193                    0.0482759       0.922414                   -51.7357  130.594\n",
       "    11       0.50004                     0.329158           0.275796   1.89985            0.0128205        0.0883154                   0.0275862       0.95                       -72.4204  89.9848\n",
       "    12       0.599984                    0.232233           0.207013   1.61786            0.0096231        0.0752071                   0.0206897       0.97069                    -79.2987  61.7859\n",
       "    13       0.700008                    0.151979           0.172372   1.41131            0.00801282       0.0656057                   0.0172414       0.987931                   -82.7628  41.1314\n",
       "    14       0.799952                    0.0817677          0.0862553  1.24576            0.00400962       0.05791                     0.00862069      0.996552                   -91.3745  24.5765\n",
       "    15       0.899976                    0.0324959          0.0172372  1.10923            0.000801282      0.0515629                   0.00172414      0.998276                   -98.2763  10.9225\n",
       "    16       1                           0.00112827         0.0172372  1                  0.000801282      0.0464855                   0.00172414      1                          -98.2763  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning (DL) - Refinement 3\n",
    "\n",
    "hyper_parameters = {'activation': ['tanh', 'maxout'],\n",
    "                    'epochs' : [5],\n",
    "                    'hidden' : [[30, 30], [50, 50]],\n",
    "                    'input_dropout_ratio' : [.1],\n",
    "                    'l1' : [.0],\n",
    "                    'l2' : [.0]\n",
    "                   }\n",
    "\n",
    "dl_grid = H2OGridSearch(H2ODeepLearningEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                        grid_id='dl_grid_3',\n",
    "                        hyper_params=hyper_parameters)\n",
    "\n",
    "dl_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "dl_gridperf1 = dl_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(dl_gridperf1)\n",
    "\n",
    "# Grab the top DL model, chosen by validation F1\n",
    "best_dl = dl_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_dl_perf1 = best_dl.model_performance(test_h2o)\n",
    "print(best_dl_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeplearning Grid Build progress: |███████████████████████████████████████| 100%\n",
      "    activation             epochs    hidden input_dropout_ratio   l1   l2  \\\n",
      "0         Tanh  5.197171810615108  [10, 10]                 0.1  0.0  0.0   \n",
      "\n",
      "           model_ids                  f1  \n",
      "0  dl_grid_4_model_0  0.8348618317847232  \n",
      "\n",
      "\n",
      "ModelMetricsBinomial: deeplearning\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.0320765761761\n",
      "RMSE: 0.179099347224\n",
      "LogLoss: 0.122198992032\n",
      "Mean Per-Class Error: 0.174675591934\n",
      "AUC: 0.905747319666\n",
      "Gini: 0.811494639332\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.408306444204: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>11719.0</td>\n",
       "<td>178.0</td>\n",
       "<td>0.015</td>\n",
       "<td> (178.0/11897.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>327.0</td>\n",
       "<td>253.0</td>\n",
       "<td>0.5638</td>\n",
       "<td> (327.0/580.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12046.0</td>\n",
       "<td>431.0</td>\n",
       "<td>0.0405</td>\n",
       "<td> (505.0/12477.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1    Error    Rate\n",
       "-----  -----  ---  -------  ---------------\n",
       "0      11719  178  0.015    (178.0/11897.0)\n",
       "1      327    253  0.5638   (327.0/580.0)\n",
       "Total  12046  431  0.0405   (505.0/12477.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4083064</td>\n",
       "<td>0.5004946</td>\n",
       "<td>136.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.0798509</td>\n",
       "<td>0.5504331</td>\n",
       "<td>274.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6587980</td>\n",
       "<td>0.5679012</td>\n",
       "<td>77.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6634303</td>\n",
       "<td>0.9621704</td>\n",
       "<td>76.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9654107</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0025218</td>\n",
       "<td>1.0</td>\n",
       "<td>395.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9654107</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4083064</td>\n",
       "<td>0.4856323</td>\n",
       "<td>136.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.0257868</td>\n",
       "<td>0.8241379</td>\n",
       "<td>332.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.0257868</td>\n",
       "<td>0.8253244</td>\n",
       "<td>332.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.408306     0.500495  136\n",
       "max f2                       0.0798509    0.550433  274\n",
       "max f0point5                 0.658798     0.567901  77\n",
       "max accuracy                 0.66343      0.96217   76\n",
       "max precision                0.965411     1         0\n",
       "max recall                   0.00252178   1         395\n",
       "max specificity              0.965411     1         0\n",
       "max absolute_mcc             0.408306     0.485632  136\n",
       "max min_per_class_accuracy   0.0257868    0.824138  332\n",
       "max mean_per_class_accuracy  0.0257868    0.825324  332"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate:  4,65 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100184</td>\n",
       "<td>0.7788110</td>\n",
       "<td>15.8328828</td>\n",
       "<td>15.8328828</td>\n",
       "<td>0.736</td>\n",
       "<td>0.736</td>\n",
       "<td>0.1586207</td>\n",
       "<td>0.1586207</td>\n",
       "<td>1483.2882759</td>\n",
       "<td>1483.2882759</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200369</td>\n",
       "<td>0.6693131</td>\n",
       "<td>14.6282069</td>\n",
       "<td>15.2305448</td>\n",
       "<td>0.68</td>\n",
       "<td>0.708</td>\n",
       "<td>0.1465517</td>\n",
       "<td>0.3051724</td>\n",
       "<td>1362.8206897</td>\n",
       "<td>1423.0544828</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300553</td>\n",
       "<td>0.4842197</td>\n",
       "<td>9.2932138</td>\n",
       "<td>13.2514345</td>\n",
       "<td>0.432</td>\n",
       "<td>0.616</td>\n",
       "<td>0.0931034</td>\n",
       "<td>0.3982759</td>\n",
       "<td>829.3213793</td>\n",
       "<td>1225.1434483</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400737</td>\n",
       "<td>0.3355950</td>\n",
       "<td>5.8512828</td>\n",
       "<td>11.4013966</td>\n",
       "<td>0.272</td>\n",
       "<td>0.53</td>\n",
       "<td>0.0586207</td>\n",
       "<td>0.4568966</td>\n",
       "<td>485.1282759</td>\n",
       "<td>1040.1396552</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500120</td>\n",
       "<td>0.2532910</td>\n",
       "<td>4.3371107</td>\n",
       "<td>9.9975962</td>\n",
       "<td>0.2016129</td>\n",
       "<td>0.4647436</td>\n",
       "<td>0.0431034</td>\n",
       "<td>0.5</td>\n",
       "<td>333.7110679</td>\n",
       "<td>899.7596154</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000240</td>\n",
       "<td>0.0797433</td>\n",
       "<td>3.5508704</td>\n",
       "<td>6.7742333</td>\n",
       "<td>0.1650641</td>\n",
       "<td>0.3149038</td>\n",
       "<td>0.1775862</td>\n",
       "<td>0.6775862</td>\n",
       "<td>255.0870358</td>\n",
       "<td>577.4233256</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500361</td>\n",
       "<td>0.0414254</td>\n",
       "<td>1.5858256</td>\n",
       "<td>5.0447640</td>\n",
       "<td>0.0737179</td>\n",
       "<td>0.2345085</td>\n",
       "<td>0.0793103</td>\n",
       "<td>0.7568966</td>\n",
       "<td>58.5825597</td>\n",
       "<td>404.4764036</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000481</td>\n",
       "<td>0.0262756</td>\n",
       "<td>1.1721320</td>\n",
       "<td>4.0766060</td>\n",
       "<td>0.0544872</td>\n",
       "<td>0.1895032</td>\n",
       "<td>0.0586207</td>\n",
       "<td>0.8155172</td>\n",
       "<td>17.2131963</td>\n",
       "<td>307.6606018</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999920</td>\n",
       "<td>0.0142183</td>\n",
       "<td>0.7245444</td>\n",
       "<td>2.9598492</td>\n",
       "<td>0.0336808</td>\n",
       "<td>0.1375902</td>\n",
       "<td>0.0724138</td>\n",
       "<td>0.8879310</td>\n",
       "<td>-27.5455576</td>\n",
       "<td>195.9849190</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000160</td>\n",
       "<td>0.0093926</td>\n",
       "<td>0.4998798</td>\n",
       "<td>2.3447336</td>\n",
       "<td>0.0232372</td>\n",
       "<td>0.1089962</td>\n",
       "<td>0.05</td>\n",
       "<td>0.9379310</td>\n",
       "<td>-50.0120192</td>\n",
       "<td>134.4733624</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000401</td>\n",
       "<td>0.0067833</td>\n",
       "<td>0.2757958</td>\n",
       "<td>1.9308797</td>\n",
       "<td>0.0128205</td>\n",
       "<td>0.0897580</td>\n",
       "<td>0.0275862</td>\n",
       "<td>0.9655172</td>\n",
       "<td>-72.4204244</td>\n",
       "<td>93.0879728</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999840</td>\n",
       "<td>0.0051282</td>\n",
       "<td>0.1207574</td>\n",
       "<td>1.6293539</td>\n",
       "<td>0.0056135</td>\n",
       "<td>0.0757414</td>\n",
       "<td>0.0120690</td>\n",
       "<td>0.9775862</td>\n",
       "<td>-87.9242596</td>\n",
       "<td>62.9353874</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000080</td>\n",
       "<td>0.0041178</td>\n",
       "<td>0.0861862</td>\n",
       "<td>1.4088509</td>\n",
       "<td>0.0040064</td>\n",
       "<td>0.0654912</td>\n",
       "<td>0.0086207</td>\n",
       "<td>0.9862069</td>\n",
       "<td>-91.3813826</td>\n",
       "<td>40.8850864</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999519</td>\n",
       "<td>0.0033919</td>\n",
       "<td>0.0517532</td>\n",
       "<td>1.2392986</td>\n",
       "<td>0.0024058</td>\n",
       "<td>0.0576095</td>\n",
       "<td>0.0051724</td>\n",
       "<td>0.9913793</td>\n",
       "<td>-94.8246827</td>\n",
       "<td>23.9298633</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999760</td>\n",
       "<td>0.0028385</td>\n",
       "<td>0.0344745</td>\n",
       "<td>1.1053935</td>\n",
       "<td>0.0016026</td>\n",
       "<td>0.0513848</td>\n",
       "<td>0.0034483</td>\n",
       "<td>0.9948276</td>\n",
       "<td>-96.5525531</td>\n",
       "<td>10.5393516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0007996</td>\n",
       "<td>0.0517117</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0024038</td>\n",
       "<td>0.0464855</td>\n",
       "<td>0.0051724</td>\n",
       "<td>1.0</td>\n",
       "<td>-94.8288296</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100184                   0.778811           15.8329    15.8329            0.736            0.736                       0.158621        0.158621                   1483.29   1483.29\n",
       "    2        0.0200369                   0.669313           14.6282    15.2305            0.68             0.708                       0.146552        0.305172                   1362.82   1423.05\n",
       "    3        0.0300553                   0.48422            9.29321    13.2514            0.432            0.616                       0.0931034       0.398276                   829.321   1225.14\n",
       "    4        0.0400737                   0.335595           5.85128    11.4014            0.272            0.53                        0.0586207       0.456897                   485.128   1040.14\n",
       "    5        0.050012                    0.253291           4.33711    9.9976             0.201613         0.464744                    0.0431034       0.5                        333.711   899.76\n",
       "    6        0.100024                    0.0797433          3.55087    6.77423            0.165064         0.314904                    0.177586        0.677586                   255.087   577.423\n",
       "    7        0.150036                    0.0414254          1.58583    5.04476            0.0737179        0.234509                    0.0793103       0.756897                   58.5826   404.476\n",
       "    8        0.200048                    0.0262756          1.17213    4.07661            0.0544872        0.189503                    0.0586207       0.815517                   17.2132   307.661\n",
       "    9        0.299992                    0.0142183          0.724544   2.95985            0.0336808        0.13759                     0.0724138       0.887931                   -27.5456  195.985\n",
       "    10       0.400016                    0.00939257         0.49988    2.34473            0.0232372        0.108996                    0.05            0.937931                   -50.012   134.473\n",
       "    11       0.50004                     0.00678334         0.275796   1.93088            0.0128205        0.089758                    0.0275862       0.965517                   -72.4204  93.088\n",
       "    12       0.599984                    0.00512817         0.120757   1.62935            0.00561347       0.0757414                   0.012069        0.977586                   -87.9243  62.9354\n",
       "    13       0.700008                    0.00411781         0.0861862  1.40885            0.00400641       0.0654912                   0.00862069      0.986207                   -91.3814  40.8851\n",
       "    14       0.799952                    0.0033919          0.0517532  1.2393             0.00240577       0.0576095                   0.00517241      0.991379                   -94.8247  23.9299\n",
       "    15       0.899976                    0.00283847         0.0344745  1.10539            0.00160256       0.0513848                   0.00344828      0.994828                   -96.5526  10.5394\n",
       "    16       1                           0.000799572        0.0517117  1                  0.00240385       0.0464855                   0.00517241      1                          -94.8288  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Deep Learning (DL) - Final\n",
    "\n",
    "hyper_parameters = {'activation': ['tanh'],\n",
    "                    'epochs' : [5],\n",
    "                    'hidden' : [[10, 10]],\n",
    "                    'input_dropout_ratio' : [.1],\n",
    "                    'l1' : [.0],\n",
    "                    'l2' : [.0]\n",
    "                   }\n",
    "\n",
    "dl_grid = H2OGridSearch(H2ODeepLearningEstimator(seed=my_seed, \n",
    "                                                 nfolds=5, \n",
    "                                                 fold_assignment='Modulo'), \n",
    "                        grid_id='dl_grid_4',\n",
    "                        hyper_params=hyper_parameters)\n",
    "\n",
    "dl_grid.train(x=features, \n",
    "               y='label', \n",
    "               training_frame=train_h2o)\n",
    "\n",
    "# Get the grid results, sorted by validation F1-Measure\n",
    "dl_gridperf1 = dl_grid.get_grid(sort_by='F1', decreasing=True)\n",
    "print(dl_gridperf1)\n",
    "\n",
    "# Grab the top DL model, chosen by validation F1\n",
    "best_dl = dl_gridperf1.models[0]\n",
    "\n",
    "# Now let's evaluate the model performance on a test set\n",
    "# so we get an honest estimate of top model performance\n",
    "best_dl_perf1 = best_dl.model_performance(test_h2o)\n",
    "print(best_dl_perf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVEqJQkREKvX/L/zfh7SgPV0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f754c803b90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_dl_perf1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_8255 closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
